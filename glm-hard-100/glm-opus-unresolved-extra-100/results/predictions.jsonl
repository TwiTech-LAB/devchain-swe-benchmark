{"instance_id": "sphinx-doc__sphinx-10614", "model_name_or_path": "devchain", "model_patch": "diff --git a/sphinx/ext/graphviz.py b/sphinx/ext/graphviz.py\nindex 8b21d9e3b..a2ec1122b 100644\n--- a/sphinx/ext/graphviz.py\n+++ b/sphinx/ext/graphviz.py\n@@ -218,7 +218,7 @@ class GraphvizSimple(SphinxDirective):\n \n \n def fix_svg_relative_paths(self: SphinxTranslator, filepath: str) -> None:\n-    \"\"\"Change relative links in generated svg files to be relative to imgpath.\"\"\"\n+    \"\"\"Change relative links in generated svg files to be relative to imagedir.\"\"\"\n     tree = ET.parse(filepath)  # NoQA: S314\n     root = tree.getroot()\n     ns = {'svg': 'http://www.w3.org/2000/svg', 'xlink': 'http://www.w3.org/1999/xlink'}\n@@ -237,7 +237,7 @@ def fix_svg_relative_paths(self: SphinxTranslator, filepath: str) -> None:\n         old_path = path.join(self.builder.outdir, url)\n         new_path = path.relpath(\n             old_path,\n-            start=path.join(self.builder.outdir, self.builder.imgpath),\n+            start=path.join(self.builder.outdir, self.builder.imagedir),\n         )\n         modified_url = urlunsplit((scheme, hostname, new_path, query, fragment))\n \ndiff --git a/sphinx/ext/inheritance_diagram.py b/sphinx/ext/inheritance_diagram.py\nindex 6e3985512..bad29ae4f 100644\n--- a/sphinx/ext/inheritance_diagram.py\n+++ b/sphinx/ext/inheritance_diagram.py\n@@ -33,6 +33,7 @@ from __future__ import annotations\n import builtins\n import hashlib\n import inspect\n+import posixpath\n import re\n from collections.abc import Iterable\n from importlib import import_module\n@@ -413,12 +414,15 @@ def html_visit_inheritance_diagram(self: HTML5Translator, node: inheritance_diag\n     for child in pending_xrefs:\n         if child.get('refuri') is not None:\n             if graphviz_output_format == 'SVG':\n-                urls[child['reftitle']] = \"../\" + child.get('refuri')\n+                # Convert refuri from document-relative to output-root-relative\n+                # so fix_svg_relative_paths can correctly resolve it from _images/\n+                current_dir = posixpath.dirname(current_filename)\n+                urls[child['reftitle']] = posixpath.join(current_dir, child.get('refuri'))\n             else:\n                 urls[child['reftitle']] = child.get('refuri')\n         elif child.get('refid') is not None:\n             if graphviz_output_format == 'SVG':\n-                urls[child['reftitle']] = '../' + current_filename + '#' + child.get('refid')\n+                urls[child['reftitle']] = current_filename + '#' + child.get('refid')\n             else:\n                 urls[child['reftitle']] = '#' + child.get('refid')\n \ndiff --git a/tox.ini b/tox.ini\nindex a3dc56211..c76147de3 100644\n--- a/tox.ini\n+++ b/tox.ini\n@@ -24,7 +24,7 @@ setenv =\n     PYTHONWARNINGS = error\n     PYTEST_ADDOPTS = {env:PYTEST_ADDOPTS:} --color yes\n commands=\n-    python -X dev -X warn_default_encoding -m pytest --durations 25 {posargs}\n+    python -X dev -X warn_default_encoding -m pytest -rA --durations 25 {posargs}\n \n [testenv:docs]\n basepython = python3\n"}
{"instance_id": "django__django-13195", "model_name_or_path": "devchain", "model_patch": "diff --git a/django/contrib/messages/storage/cookie.py b/django/contrib/messages/storage/cookie.py\nindex 78256ca3b2..b51e292aa0 100644\n--- a/django/contrib/messages/storage/cookie.py\n+++ b/django/contrib/messages/storage/cookie.py\n@@ -92,7 +92,11 @@ class CookieStorage(BaseStorage):\n                 samesite=settings.SESSION_COOKIE_SAMESITE,\n             )\n         else:\n-            response.delete_cookie(self.cookie_name, domain=settings.SESSION_COOKIE_DOMAIN)\n+            response.delete_cookie(\n+                self.cookie_name,\n+                domain=settings.SESSION_COOKIE_DOMAIN,\n+                samesite=settings.SESSION_COOKIE_SAMESITE,\n+            )\n \n     def _store(self, messages, response, remove_oldest=True, *args, **kwargs):\n         \"\"\"\ndiff --git a/django/contrib/sessions/middleware.py b/django/contrib/sessions/middleware.py\nindex 63013eef7a..95ad30ce7f 100644\n--- a/django/contrib/sessions/middleware.py\n+++ b/django/contrib/sessions/middleware.py\n@@ -42,6 +42,7 @@ class SessionMiddleware(MiddlewareMixin):\n                 settings.SESSION_COOKIE_NAME,\n                 path=settings.SESSION_COOKIE_PATH,\n                 domain=settings.SESSION_COOKIE_DOMAIN,\n+                samesite=settings.SESSION_COOKIE_SAMESITE,\n             )\n             patch_vary_headers(response, ('Cookie',))\n         else:\ndiff --git a/django/http/response.py b/django/http/response.py\nindex e00bcacefb..b9ad30c5ea 100644\n--- a/django/http/response.py\n+++ b/django/http/response.py\n@@ -210,13 +210,13 @@ class HttpResponseBase:\n         value = signing.get_cookie_signer(salt=key + salt).sign(value)\n         return self.set_cookie(key, value, **kwargs)\n \n-    def delete_cookie(self, key, path='/', domain=None):\n+    def delete_cookie(self, key, path='/', domain=None, samesite=None):\n         # Most browsers ignore the Set-Cookie header if the cookie name starts\n         # with __Host- or __Secure- and the cookie doesn't use the secure flag.\n         secure = key.startswith(('__Secure-', '__Host-'))\n         self.set_cookie(\n             key, max_age=0, path=path, domain=domain, secure=secure,\n-            expires='Thu, 01 Jan 1970 00:00:00 GMT',\n+            expires='Thu, 01 Jan 1970 00:00:00 GMT', samesite=samesite,\n         )\n \n     # Common methods used by subclasses\n"}
{"instance_id": "django__django-12308", "model_name_or_path": "devchain", "model_patch": "diff --git a/django/contrib/admin/utils.py b/django/contrib/admin/utils.py\nindex e4ddc8f017..90d14adfd4 100644\n--- a/django/contrib/admin/utils.py\n+++ b/django/contrib/admin/utils.py\n@@ -1,5 +1,6 @@\n import datetime\n import decimal\n+import json\n from collections import defaultdict\n \n from django.core.exceptions import FieldDoesNotExist\n@@ -398,6 +399,11 @@ def display_for_field(value, field, empty_value_display):\n         return formats.number_format(value)\n     elif isinstance(field, models.FileField) and value:\n         return format_html('<a href=\"{}\">{}</a>', value.url, value)\n+    elif isinstance(field, models.JSONField):\n+        try:\n+            return json.dumps(value, cls=field.encoder, ensure_ascii=False)\n+        except TypeError:\n+            return display_for_value(value, empty_value_display)\n     else:\n         return display_for_value(value, empty_value_display)\n \n"}
{"instance_id": "django__django-10999", "model_name_or_path": "devchain", "model_patch": "diff --git a/django/utils/dateparse.py b/django/utils/dateparse.py\nindex 8d08b7d1d3..a037758784 100644\n--- a/django/utils/dateparse.py\n+++ b/django/utils/dateparse.py\n@@ -29,7 +29,7 @@ datetime_re = re.compile(\n standard_duration_re = re.compile(\n     r'^'\n     r'(?:(?P<days>-?\\d+) (days?, )?)?'\n-    r'((?:(?P<hours>-?\\d+):)(?=\\d+:\\d+))?'\n+    r'((?:(?P<hours>-?\\d+):)(?=-?\\d+:-?\\d+))?'\n     r'(?:(?P<minutes>-?\\d+):)?'\n     r'(?P<seconds>-?\\d+)'\n     r'(?:\\.(?P<microseconds>\\d{1,6})\\d{0,6})?'\n"}
{"instance_id": "django__django-11141", "model_name_or_path": "devchain", "model_patch": "diff --git a/django/db/migrations/loader.py b/django/db/migrations/loader.py\nindex 8c308621d2..2761aff2d4 100644\n--- a/django/db/migrations/loader.py\n+++ b/django/db/migrations/loader.py\n@@ -84,11 +84,6 @@ class MigrationLoader:\n                     continue\n                 raise\n             else:\n-                # Empty directories are namespaces.\n-                # getattr() needed on PY36 and older (replace w/attribute access).\n-                if getattr(module, '__file__', None) is None:\n-                    self.unmigrated_apps.add(app_config.label)\n-                    continue\n                 # Module is not a package (e.g. migrations.py).\n                 if not hasattr(module, '__path__'):\n                     self.unmigrated_apps.add(app_config.label)\n@@ -101,6 +96,12 @@ class MigrationLoader:\n                 name for _, name, is_pkg in pkgutil.iter_modules(module.__path__)\n                 if not is_pkg and name[0] not in '_~'\n             }\n+            # Empty directories are namespaces. If no migrations are found,\n+            # treat the app as unmigrated.\n+            if not migration_names and getattr(module, '__file__', None) is None:\n+                self.migrated_apps.discard(app_config.label)\n+                self.unmigrated_apps.add(app_config.label)\n+                continue\n             # Load migrations\n             for migration_name in migration_names:\n                 migration_path = '%s.%s' % (module_name, migration_name)\n"}
{"instance_id": "pydata__xarray-3993", "model_name_or_path": "devchain", "model_patch": "diff --git a/xarray/core/dataarray.py b/xarray/core/dataarray.py\nindex 2fef3edb..3b7e2b8e 100644\n--- a/xarray/core/dataarray.py\n+++ b/xarray/core/dataarray.py\n@@ -3481,21 +3481,27 @@ class DataArray(AbstractArray, DataWithCoords):\n         return self._from_temp_dataset(ds)\n \n     def integrate(\n-        self, dim: Union[Hashable, Sequence[Hashable]], datetime_unit: str = None\n+        self,\n+        coord: Union[Hashable, Sequence[Hashable], None] = None,\n+        datetime_unit: str = None,\n+        *,\n+        dim: Union[Hashable, Sequence[Hashable], None] = None,\n     ) -> \"DataArray\":\n         \"\"\" integrate the array with the trapezoidal rule.\n \n         .. note::\n-            This feature is limited to simple cartesian geometry, i.e. dim\n+            This feature is limited to simple cartesian geometry, i.e. coord\n             must be one dimensional.\n \n         Parameters\n         ----------\n-        dim : hashable, or sequence of hashable\n+        coord : hashable, or sequence of hashable\n             Coordinate(s) used for the integration.\n         datetime_unit : {\"Y\", \"M\", \"W\", \"D\", \"h\", \"m\", \"s\", \"ms\", \"us\", \"ns\", \\\n                          \"ps\", \"fs\", \"as\"}, optional\n             Can be used to specify the unit if datetime coordinate is used.\n+        dim : hashable, or sequence of hashable, deprecated\n+            Use ``coord`` instead.\n \n         Returns\n         -------\n@@ -3528,7 +3534,21 @@ class DataArray(AbstractArray, DataWithCoords):\n         array([5.4, 6.6, 7.8])\n         Dimensions without coordinates: y\n         \"\"\"\n-        ds = self._to_temp_dataset().integrate(dim, datetime_unit)\n+        if dim is not None:\n+            if coord is not None:\n+                raise ValueError(\n+                    \"Cannot supply both 'coord' and 'dim' arguments to integrate.\"\n+                )\n+            warnings.warn(\n+                \"The `dim` argument to `DataArray.integrate` is being renamed to `coord`. \"\n+                \"Please use `coord` instead. `dim` will be removed in a future version.\",\n+                FutureWarning,\n+                stacklevel=2,\n+            )\n+            coord = dim\n+        if coord is None:\n+            raise ValueError(\"Must supply 'coord' argument to integrate.\")\n+        ds = self._to_temp_dataset().integrate(coord, datetime_unit)\n         return self._from_temp_dataset(ds)\n \n     def unify_chunks(self) -> \"DataArray\":\n"}
{"instance_id": "django__django-15732", "model_name_or_path": "devchain", "model_patch": "diff --git a/django/db/backends/base/schema.py b/django/db/backends/base/schema.py\nindex f2ca8c8df9..b3f9e7659a 100644\n--- a/django/db/backends/base/schema.py\n+++ b/django/db/backends/base/schema.py\n@@ -528,7 +528,7 @@ class BaseDatabaseSchemaEditor:\n         # Deleted uniques\n         for fields in olds.difference(news):\n             self._delete_composed_index(\n-                model, fields, {\"unique\": True}, self.sql_delete_unique\n+                model, fields, {\"unique\": True, \"primary_key\": False}, self.sql_delete_unique\n             )\n         # Created uniques\n         for field_names in news.difference(olds):\n"}
{"instance_id": "pylint-dev__pylint-7080", "model_name_or_path": "devchain", "model_patch": "diff --git a/pylint/lint/expand_modules.py b/pylint/lint/expand_modules.py\nindex 5cacc0371..83e59bade 100644\n--- a/pylint/lint/expand_modules.py\n+++ b/pylint/lint/expand_modules.py\n@@ -51,12 +51,26 @@ def _is_ignored_file(\n     ignore_list: list[str],\n     ignore_list_re: list[Pattern[str]],\n     ignore_list_paths_re: list[Pattern[str]],\n+    element_is_directory: bool = False,\n ) -> bool:\n+    \"\"\"Check if a file or directory element should be ignored.\n+\n+    When element_is_directory is True, the element is treated as a directory\n+    and a trailing separator is added when checking against ignore_list_paths_re.\n+    This is necessary because ignore-paths patterns often expect directory\n+    paths to end with a separator (e.g., '^src/gen/.*$' to match 'src/gen/').\n+    \"\"\"\n     basename = os.path.basename(element)\n+    # When checking if a directory should be ignored by path patterns,\n+    # add a trailing separator to match patterns like '^src/gen/.*$'\n+    element_for_path_check = (\n+        element if not element_is_directory or element.endswith(os.sep)\n+        else element + os.sep\n+    )\n     return (\n         basename in ignore_list\n         or _is_in_ignore_list_re(basename, ignore_list_re)\n-        or _is_in_ignore_list_re(element, ignore_list_paths_re)\n+        or _is_in_ignore_list_re(element_for_path_check, ignore_list_paths_re)\n     )\n \n \ndiff --git a/pylint/lint/pylinter.py b/pylint/lint/pylinter.py\nindex e2bbf061e..23507e003 100644\n--- a/pylint/lint/pylinter.py\n+++ b/pylint/lint/pylinter.py\n@@ -602,6 +602,7 @@ class PyLinter(\n                         self.config.ignore,\n                         self.config.ignore_patterns,\n                         self.config.ignore_paths,\n+                        element_is_directory=True,\n                     ):\n                         skip_subtrees.append(root)\n                         continue\n"}
{"instance_id": "django__django-11477", "model_name_or_path": "devchain", "model_patch": "diff --git a/django/urls/resolvers.py b/django/urls/resolvers.py\nindex af0508f94e..7cbe3e7e45 100644\n--- a/django/urls/resolvers.py\n+++ b/django/urls/resolvers.py\n@@ -153,7 +153,7 @@ class RegexPattern(CheckURLMixin):\n             # If there are any named groups, use those as kwargs, ignoring\n             # non-named groups. Otherwise, pass all non-named arguments as\n             # positional arguments.\n-            kwargs = match.groupdict()\n+            kwargs = {k: v for k, v in match.groupdict().items() if v is not None}\n             args = () if kwargs else match.groups()\n             return path[match.end():], args, kwargs\n         return None\n@@ -249,7 +249,7 @@ class RoutePattern(CheckURLMixin):\n         match = self.regex.search(path)\n         if match:\n             # RoutePattern doesn't allow non-named groups so args are ignored.\n-            kwargs = match.groupdict()\n+            kwargs = {k: v for k, v in match.groupdict().items() if v is not None}\n             for key, value in kwargs.items():\n                 converter = self.converters[key]\n                 try:\n"}
{"instance_id": "matplotlib__matplotlib-25960", "model_name_or_path": "devchain", "model_patch": "diff --git a/lib/matplotlib/figure.py b/lib/matplotlib/figure.py\nindex dab2bc4a5a..8269a98034 100644\n--- a/lib/matplotlib/figure.py\n+++ b/lib/matplotlib/figure.py\n@@ -2274,17 +2274,70 @@ class SubFigure(FigureBase):\n             return\n         # need to figure out *where* this subplotspec is.\n         gs = self._subplotspec.get_gridspec()\n+        nrows, ncols = gs.get_geometry()\n+\n+        # For constrained layout, don't apply margins or spacing\n+        if self.get_constrained_layout():\n+            left = 0.\n+            right = 1.\n+            bottom = 0.\n+            top = 1.\n+            wspace = 0.\n+            hspace = 0.\n+        else:\n+            # Get subplot parameters from figure and GridSpec\n+            subplot_params = gs.get_subplot_params(self.figure)\n+            left = subplot_params.left\n+            right = subplot_params.right\n+            bottom = subplot_params.bottom\n+            top = subplot_params.top\n+            wspace = subplot_params.wspace\n+            hspace = subplot_params.hspace\n+\n+        tot_width = right - left\n+        tot_height = top - bottom\n+\n+        # Calculate grid positions accounting for wspace and hspace\n         wr = np.asarray(gs.get_width_ratios())\n         hr = np.asarray(gs.get_height_ratios())\n-        dx = wr[self._subplotspec.colspan].sum() / wr.sum()\n-        dy = hr[self._subplotspec.rowspan].sum() / hr.sum()\n-        x0 = wr[:self._subplotspec.colspan.start].sum() / wr.sum()\n-        y0 = 1 - hr[:self._subplotspec.rowspan.stop].sum() / hr.sum()\n+\n+        # calculate accumulated heights\n+        cell_h = tot_height / (nrows + hspace * (nrows - 1))\n+        sep_h = hspace * cell_h\n+        norm = cell_h * nrows / sum(hr)\n+        cell_heights = [r * norm for r in hr]\n+        sep_heights = [0] + ([sep_h] * (nrows - 1))\n+        cell_hs = np.cumsum(np.column_stack([sep_heights, cell_heights]).flat)\n+        # .T swaps the rows and columns after reshape\n+        fig_tops, fig_bottoms = (top - cell_hs).reshape((-1, 2)).T\n+\n+        # calculate accumulated widths\n+        cell_w = tot_width / (ncols + wspace * (ncols - 1))\n+        sep_w = wspace * cell_w\n+        norm = cell_w * ncols / sum(wr)\n+        cell_widths = [r * norm for r in wr]\n+        sep_widths = [0] + ([sep_w] * (ncols - 1))\n+        cell_ws = np.cumsum(np.column_stack([sep_widths, cell_widths]).flat)\n+        # .T swaps the rows and columns after reshape\n+        fig_lefts, fig_rights = (left + cell_ws).reshape((-1, 2)).T\n+\n+        # Get the position for this subplotspec's rows and columns\n+        rows = np.arange(self._subplotspec.rowspan.start,\n+                         self._subplotspec.rowspan.stop)\n+        cols = np.arange(self._subplotspec.colspan.start,\n+                         self._subplotspec.colspan.stop)\n+\n+        fig_bottom = fig_bottoms[rows].min()\n+        fig_top = fig_tops[rows].max()\n+        fig_left = fig_lefts[cols].min()\n+        fig_right = fig_rights[cols].max()\n+\n         if self.bbox_relative is None:\n-            self.bbox_relative = Bbox.from_bounds(x0, y0, dx, dy)\n+            self.bbox_relative = Bbox.from_extents(\n+                fig_left, fig_bottom, fig_right, fig_top)\n         else:\n-            self.bbox_relative.p0 = (x0, y0)\n-            self.bbox_relative.p1 = (x0 + dx, y0 + dy)\n+            self.bbox_relative.p0 = (fig_left, fig_bottom)\n+            self.bbox_relative.p1 = (fig_right, fig_top)\n \n     def get_constrained_layout(self):\n         \"\"\"\n"}
{"instance_id": "scikit-learn__scikit-learn-12973", "model_name_or_path": "devchain", "model_patch": "diff --git a/sklearn/linear_model/least_angle.py b/sklearn/linear_model/least_angle.py\nindex 03b34a0dc..20a2a06ef 100644\n--- a/sklearn/linear_model/least_angle.py\n+++ b/sklearn/linear_model/least_angle.py\n@@ -1479,7 +1479,7 @@ class LassoLarsIC(LassoLars):\n         self.eps = eps\n         self.fit_path = True\n \n-    def fit(self, X, y, copy_X=True):\n+    def fit(self, X, y, copy_X=None):\n         \"\"\"Fit the model using X, y as training data.\n \n         Parameters\n@@ -1492,6 +1492,7 @@ class LassoLarsIC(LassoLars):\n \n         copy_X : boolean, optional, default True\n             If ``True``, X will be copied; else, it may be overwritten.\n+            If ``None``, the value set during initialization is used.\n \n         Returns\n         -------\n@@ -1500,8 +1501,11 @@ class LassoLarsIC(LassoLars):\n         \"\"\"\n         X, y = check_X_y(X, y, y_numeric=True)\n \n+        if copy_X is None:\n+            copy_X = self.copy_X\n+\n         X, y, Xmean, ymean, Xstd = LinearModel._preprocess_data(\n-            X, y, self.fit_intercept, self.normalize, self.copy_X)\n+            X, y, self.fit_intercept, self.normalize, copy_X)\n         max_iter = self.max_iter\n \n         Gram = self.precompute\n"}
{"instance_id": "matplotlib__matplotlib-23476", "model_name_or_path": "devchain", "model_patch": "diff --git a/lib/matplotlib/figure.py b/lib/matplotlib/figure.py\nindex c55864243a..84a2dcc180 100644\n--- a/lib/matplotlib/figure.py\n+++ b/lib/matplotlib/figure.py\n@@ -3030,11 +3030,22 @@ class Figure(FigureBase):\n         from matplotlib import _pylab_helpers\n         if self.canvas.manager in _pylab_helpers.Gcf.figs.values():\n             state['_restore_to_pylab'] = True\n+\n+        # Store the device pixel ratio so we can correctly restore _original_dpi\n+        # on unpickling. This prevents DPI doubling when unpickling figures with\n+        # non-default device pixel ratios (e.g., on retina displays).\n+        if hasattr(self.canvas, \"_device_pixel_ratio\"):\n+            state[\"_device_pixel_ratio\"] = self.canvas._device_pixel_ratio\n+\n+        # Don't pickle _original_dpi so it gets recalculated on unpickling.\n+        state.pop(\"_original_dpi\", None)\n+\n         return state\n \n     def __setstate__(self, state):\n         version = state.pop('__mpl_version__')\n         restore_to_pylab = state.pop('_restore_to_pylab', False)\n+        device_pixel_ratio = state.pop(\"_device_pixel_ratio\", None)\n \n         if version != mpl.__version__:\n             _api.warn_external(\n@@ -3056,7 +3067,15 @@ class Figure(FigureBase):\n             pylab_helpers.Gcf._set_new_active_manager(mgr)\n             plt.draw_if_interactive()\n \n-        self.stale = True\n+        # Restore _original_dpi to account for the device pixel ratio that was\n+        # applied to the saved DPI. This must be done AFTER all canvas creation\n+        # is complete (including the restore_to_pylab block) to prevent our\n+        # changes from being overwritten.\n+        if device_pixel_ratio is not None:\n+            self._original_dpi = self.dpi / device_pixel_ratio\n+            self.canvas._device_pixel_ratio = device_pixel_ratio\n+\n+        self._stale = True\n \n     def add_axobserver(self, func):\n         \"\"\"Whenever the Axes state change, ``func(self)`` will be called.\"\"\"\n"}
{"instance_id": "django__django-12741", "model_name_or_path": "devchain", "model_patch": "diff --git a/django/core/management/commands/flush.py b/django/core/management/commands/flush.py\nindex e8ed3967f7..bc8021cf4e 100644\n--- a/django/core/management/commands/flush.py\n+++ b/django/core/management/commands/flush.py\n@@ -60,7 +60,7 @@ Are you sure you want to do this?\n \n         if confirm == 'yes':\n             try:\n-                connection.ops.execute_sql_flush(database, sql_list)\n+                connection.ops.execute_sql_flush(sql_list)\n             except Exception as exc:\n                 raise CommandError(\n                     \"Database %s couldn't be flushed. Possible reasons:\\n\"\ndiff --git a/django/db/backends/base/operations.py b/django/db/backends/base/operations.py\nindex 70ac07ae09..f07580c18b 100644\n--- a/django/db/backends/base/operations.py\n+++ b/django/db/backends/base/operations.py\n@@ -400,9 +400,9 @@ class BaseDatabaseOperations:\n         \"\"\"\n         raise NotImplementedError('subclasses of BaseDatabaseOperations must provide a sql_flush() method')\n \n-    def execute_sql_flush(self, using, sql_list):\n+    def execute_sql_flush(self, sql_list):\n         \"\"\"Execute a list of SQL statements to flush the database.\"\"\"\n-        with transaction.atomic(using=using, savepoint=self.connection.features.can_rollback_ddl):\n+        with transaction.atomic(using=self.connection.alias, savepoint=self.connection.features.can_rollback_ddl):\n             with self.connection.cursor() as cursor:\n                 for sql in sql_list:\n                     cursor.execute(sql)\n"}
{"instance_id": "django__django-15022", "model_name_or_path": "devchain", "model_patch": "diff --git a/django/contrib/admin/options.py b/django/contrib/admin/options.py\nindex 66f3396a6b..18ab249c84 100644\n--- a/django/contrib/admin/options.py\n+++ b/django/contrib/admin/options.py\n@@ -1031,6 +1031,7 @@ class ModelAdmin(BaseModelAdmin):\n         if search_fields and search_term:\n             orm_lookups = [construct_search(str(search_field))\n                            for search_field in search_fields]\n+            search_conditions = []\n             for bit in smart_split(search_term):\n                 if bit.startswith(('\"', \"'\")) and bit[0] == bit[-1]:\n                     bit = unescape_string_literal(bit)\n@@ -1038,7 +1039,10 @@ class ModelAdmin(BaseModelAdmin):\n                     *((orm_lookup, bit) for orm_lookup in orm_lookups),\n                     _connector=models.Q.OR,\n                 )\n-                queryset = queryset.filter(or_queries)\n+                search_conditions.append(or_queries)\n+            # Apply all search conditions in a single filter() call to avoid\n+            # redundant JOINs when searching across related fields.\n+            queryset = queryset.filter(*search_conditions)\n             may_have_duplicates |= any(\n                 lookup_spawns_duplicates(self.opts, search_spec)\n                 for search_spec in orm_lookups\n"}
{"instance_id": "django__django-11087", "model_name_or_path": "devchain", "model_patch": "diff --git a/django/db/models/deletion.py b/django/db/models/deletion.py\nindex 3a462f1c4c..436fb11653 100644\n--- a/django/db/models/deletion.py\n+++ b/django/db/models/deletion.py\n@@ -234,7 +234,7 @@ class Collector:\n         \"\"\"\n         return related.related_model._base_manager.using(self.using).filter(\n             **{\"%s__in\" % related.field.name: objs}\n-        )\n+        ).only(related.related_model._meta.pk.name)\n \n     def instances_with_model(self):\n         for model, instances in self.data.items():\n"}
{"instance_id": "pylint-dev__pylint-4604", "model_name_or_path": "devchain", "model_patch": "diff --git a/pylint/checkers/variables.py b/pylint/checkers/variables.py\nindex 3ddafd599..8a4637509 100644\n--- a/pylint/checkers/variables.py\n+++ b/pylint/checkers/variables.py\n@@ -1826,6 +1826,10 @@ class VariablesChecker(BaseChecker):\n             self._type_annotation_names.append(type_annotation.name)\n             return\n \n+        if isinstance(type_annotation, astroid.Attribute):\n+            self._store_type_annotation_node(type_annotation.expr)\n+            return\n+\n         if not isinstance(type_annotation, astroid.Subscript):\n             return\n \ndiff --git a/test_a.py b/test_a.py\nnew file mode 100644\nindex 000000000..0df881939\n--- /dev/null\n+++ b/test_a.py\n@@ -0,0 +1,7 @@\n+\"\"\"Docstring.\"\"\"\n+\n+import abc\n+from abc import ABC\n+\n+X = ...  # type: abc.ABC\n+Y = ...  # type: ABC\n"}
{"instance_id": "django__django-15252", "model_name_or_path": "devchain", "model_patch": "diff --git a/django/db/migrations/recorder.py b/django/db/migrations/recorder.py\nindex 1a37c6b7d0..8ad0bf80bb 100644\n--- a/django/db/migrations/recorder.py\n+++ b/django/db/migrations/recorder.py\n@@ -1,5 +1,5 @@\n from django.apps.registry import Apps\n-from django.db import DatabaseError, models\n+from django.db import DatabaseError, models, router\n from django.utils.functional import classproperty\n from django.utils.timezone import now\n \n@@ -62,6 +62,9 @@ class MigrationRecorder:\n         # in the codebase.\n         if self.has_table():\n             return\n+        # Check if the router allows migration for this model on this database.\n+        if not router.allow_migrate_model(self.connection.alias, self.Migration):\n+            return\n         # Make the table\n         try:\n             with self.connection.schema_editor() as editor:\n"}
{"instance_id": "matplotlib__matplotlib-21568", "model_name_or_path": "devchain", "model_patch": "diff --git a/lib/matplotlib/dates.py b/lib/matplotlib/dates.py\nindex 0766156d64..69f5890629 100644\n--- a/lib/matplotlib/dates.py\n+++ b/lib/matplotlib/dates.py\n@@ -595,8 +595,11 @@ def _wrap_in_tex(text):\n     p = r'([a-zA-Z]+)'\n     ret_text = re.sub(p, r'}$\\1$\\\\mathdefault{', text)\n \n-    # Braces ensure dashes are not spaced like binary operators.\n-    ret_text = '$\\\\mathdefault{'+ret_text.replace('-', '{-}')+'}$'\n+    # Braces ensure symbols are not spaced like binary operators.\n+    ret_text = ret_text.replace('-', '{-}').replace(':', '{:}')\n+    # To not concatenate space between numbers.\n+    ret_text = ret_text.replace(' ', r'\\;')\n+    ret_text = '$\\\\mathdefault{' + ret_text + '}$'\n     ret_text = ret_text.replace('$\\\\mathdefault{}$', '')\n     return ret_text\n \n"}
{"instance_id": "sympy__sympy-13877", "model_name_or_path": "devchain", "model_patch": "diff --git a/sympy/core/exprtools.py b/sympy/core/exprtools.py\nindex 4e0de9ec18..14469c353d 100644\n--- a/sympy/core/exprtools.py\n+++ b/sympy/core/exprtools.py\n@@ -1176,7 +1176,7 @@ def do(expr):\n         if p.is_Add:\n             list_args = [do(a) for a in Add.make_args(p)]\n             # get a common negative (if there) which gcd_terms does not remove\n-            if all(a.as_coeff_Mul()[0] < 0 for a in list_args):\n+            if all(a.as_coeff_Mul()[0].is_negative for a in list_args):\n                 cont = -cont\n                 list_args = [-a for a in list_args]\n             # watch out for exp(-(x+2)) which gcd_terms will change to exp(-x-2)\n"}
{"instance_id": "sympy__sympy-17630", "model_name_or_path": "devchain", "model_patch": "diff --git a/sympy/matrices/expressions/blockmatrix.py b/sympy/matrices/expressions/blockmatrix.py\nindex 95302a05ca..83108a43b5 100644\n--- a/sympy/matrices/expressions/blockmatrix.py\n+++ b/sympy/matrices/expressions/blockmatrix.py\n@@ -164,7 +164,15 @@ def structurally_equal(self, other):\n     def _blockmul(self, other):\n         if (isinstance(other, BlockMatrix) and\n                 self.colblocksizes == other.rowblocksizes):\n-            return BlockMatrix(self.blocks*other.blocks)\n+            from sympy.core.numbers import Zero\n+            from sympy.matrices.immutable import ImmutableDenseMatrix\n+\n+            blocks = self.blocks * other.blocks\n+            # Replace scalar Zero objects with ZeroMatrix of appropriate dimensions\n+            new_blocks = ImmutableDenseMatrix(blocks.rows, blocks.cols,\n+                lambda i, j: (ZeroMatrix(self.blocks[i, 0].rows, other.blocks[0, j].cols)\n+                             if isinstance(blocks[i, j], Zero) else blocks[i, j]))\n+            return BlockMatrix(new_blocks)\n \n         return self * other\n \n"}
{"instance_id": "sympy__sympy-18698", "model_name_or_path": "devchain", "model_patch": "diff --git a/sympy/polys/polytools.py b/sympy/polys/polytools.py\nindex c9e77a901f..4d8264f6bb 100644\n--- a/sympy/polys/polytools.py\n+++ b/sympy/polys/polytools.py\n@@ -5890,6 +5890,27 @@ def key(obj):\n     return sorted(factors, key=key)\n \n \n+def _group_factors(factors):\n+    \"\"\"Group factors with the same multiplicity. \"\"\"\n+    if not factors:\n+        return []\n+\n+    result = []\n+    current_exp = factors[0][1]\n+    current_poly = factors[0][0]\n+\n+    for poly, exp in factors[1:]:\n+        if exp == current_exp:\n+            current_poly = current_poly.mul(poly)\n+        else:\n+            result.append((current_poly, current_exp))\n+            current_poly = poly\n+            current_exp = exp\n+\n+    result.append((current_poly, current_exp))\n+    return result\n+\n+\n def _factors_product(factors):\n     \"\"\"Multiply a list of ``(expr, exp)`` pairs. \"\"\"\n     return Mul(*[f.as_expr()**k for f, k in factors])\n@@ -5998,6 +6019,10 @@ def _generic_factor_list(expr, gens, args, method):\n         fp = _sorted_factors(fp, method)\n         fq = _sorted_factors(fq, method)\n \n+        if method == 'sqf':\n+            fp = _group_factors(fp)\n+            fq = _group_factors(fq)\n+\n         if not opt.polys:\n             fp = [(f.as_expr(), k) for f, k in fp]\n             fq = [(f.as_expr(), k) for f, k in fq]\n"}
{"instance_id": "django__django-13401", "model_name_or_path": "devchain", "model_patch": "diff --git a/django/db/models/fields/__init__.py b/django/db/models/fields/__init__.py\nindex 28374272f4..14ecdab505 100644\n--- a/django/db/models/fields/__init__.py\n+++ b/django/db/models/fields/__init__.py\n@@ -516,17 +516,24 @@ class Field(RegisterLookupMixin):\n     def __eq__(self, other):\n         # Needed for @total_ordering\n         if isinstance(other, Field):\n-            return self.creation_counter == other.creation_counter\n+            # Fields are equal only if they have the same creation_counter\n+            # and belong to the same model.\n+            return self.creation_counter == other.creation_counter and self.model is other.model\n         return NotImplemented\n \n     def __lt__(self, other):\n         # This is needed because bisect does not take a comparison function.\n         if isinstance(other, Field):\n-            return self.creation_counter < other.creation_counter\n+            # Order by creation_counter first to maintain existing ordering\n+            # for most cases. Fall back to comparing models when counters are equal.\n+            if self.creation_counter != other.creation_counter:\n+                return self.creation_counter < other.creation_counter\n+            # creation_counters are equal, compare by model identity\n+            return id(self.model) < id(other.model)\n         return NotImplemented\n \n     def __hash__(self):\n-        return hash(self.creation_counter)\n+        return hash((self.creation_counter, id(self.model)))\n \n     def __deepcopy__(self, memodict):\n         # We don't have to deepcopy very much here, since most things are not\n"}
{"instance_id": "astropy__astropy-14369", "model_name_or_path": "devchain", "model_patch": "diff --git a/astropy/units/format/cds.py b/astropy/units/format/cds.py\nindex 307e987ed6..dd70fc9820 100644\n--- a/astropy/units/format/cds.py\n+++ b/astropy/units/format/cds.py\n@@ -164,30 +164,56 @@ class CDS(Base):\n \n         def p_combined_units(p):\n             \"\"\"\n-            combined_units : product_of_units\n-                           | division_of_units\n+            combined_units : division_list\n+                           | product_of_units\n             \"\"\"\n             p[0] = p[1]\n \n         def p_product_of_units(p):\n             \"\"\"\n-            product_of_units : unit_expression PRODUCT combined_units\n-                             | unit_expression\n+            product_of_units : unit_with_power PRODUCT product_of_units\n+                             | unit_with_power\n             \"\"\"\n             if len(p) == 4:\n                 p[0] = p[1] * p[3]\n             else:\n                 p[0] = p[1]\n \n-        def p_division_of_units(p):\n+        def p_division_list(p):\n             \"\"\"\n-            division_of_units : DIVISION unit_expression\n-                              | unit_expression DIVISION combined_units\n+            division_list : unit_expression division_sequence\n+                          | DIVISION division_sequence\n             \"\"\"\n             if len(p) == 3:\n+                # unit_expression division_sequence\n+                # First unit is numerator, rest are accumulated in denominator\n+                numerator = p[1]\n+                denominator = p[2]\n+                p[0] = numerator / denominator\n+            else:\n+                # DIVISION division_sequence\n+                # All units are in denominator\n                 p[0] = p[2] ** -1\n+\n+        def p_division_sequence(p):\n+            \"\"\"\n+            division_sequence : DIVISION unit_expression division_sequence\n+                              | DIVISION OPEN_PAREN combined_units CLOSE_PAREN division_sequence\n+                              | DIVISION product_of_units\n+                              | DIVISION OPEN_PAREN combined_units CLOSE_PAREN\n+            \"\"\"\n+            if len(p) == 4:\n+                # DIVISION unit_expression division_sequence\n+                p[0] = p[2] * p[3]\n+            elif len(p) == 6:\n+                # DIVISION OPEN_PAREN combined_units CLOSE_PAREN division_sequence\n+                p[0] = p[3] * p[5]\n+            elif len(p) == 3:\n+                # DIVISION product_of_units\n+                p[0] = p[2]\n             else:\n-                p[0] = p[1] / p[3]\n+                # DIVISION OPEN_PAREN combined_units CLOSE_PAREN (len == 5)\n+                p[0] = p[3]\n \n         def p_unit_expression(p):\n             \"\"\"\ndiff --git a/astropy/units/format/cds_parsetab.py b/astropy/units/format/cds_parsetab.py\nindex 741d41643c..7745577855 100644\n--- a/astropy/units/format/cds_parsetab.py\n+++ b/astropy/units/format/cds_parsetab.py\n@@ -17,9 +17,9 @@ _tabversion = '3.10'\n \n _lr_method = 'LALR'\n \n-_lr_signature = 'CLOSE_BRACKET CLOSE_PAREN DIMENSIONLESS DIVISION OPEN_BRACKET OPEN_PAREN PRODUCT SIGN UFLOAT UINT UNIT X\\n            main : factor combined_units\\n                 | combined_units\\n                 | DIMENSIONLESS\\n                 | OPEN_BRACKET combined_units CLOSE_BRACKET\\n                 | OPEN_BRACKET DIMENSIONLESS CLOSE_BRACKET\\n                 | factor\\n            \\n            combined_units : product_of_units\\n                           | division_of_units\\n            \\n            product_of_units : unit_expression PRODUCT combined_units\\n                             | unit_expression\\n            \\n            division_of_units : DIVISION unit_expression\\n                              | unit_expression DIVISION combined_units\\n            \\n            unit_expression : unit_with_power\\n                            | OPEN_PAREN combined_units CLOSE_PAREN\\n            \\n            factor : signed_float X UINT signed_int\\n                   | UINT X UINT signed_int\\n                   | UINT signed_int\\n                   | UINT\\n                   | signed_float\\n            \\n            unit_with_power : UNIT numeric_power\\n                            | UNIT\\n            \\n            numeric_power : sign UINT\\n            \\n            sign : SIGN\\n                 |\\n            \\n            signed_int : SIGN UINT\\n            \\n            signed_float : sign UINT\\n                         | sign UFLOAT\\n            '\n+_lr_signature = 'CLOSE_BRACKET CLOSE_PAREN DIMENSIONLESS DIVISION OPEN_BRACKET OPEN_PAREN PRODUCT SIGN UFLOAT UINT UNIT X\\n            main : factor combined_units\\n                 | combined_units\\n                 | DIMENSIONLESS\\n                 | OPEN_BRACKET combined_units CLOSE_BRACKET\\n                 | OPEN_BRACKET DIMENSIONLESS CLOSE_BRACKET\\n                 | factor\\n            \\n            combined_units : division_list\\n                           | product_of_units\\n            \\n            product_of_units : unit_with_power PRODUCT product_of_units\\n                             | unit_with_power\\n            \\n            division_list : unit_expression division_sequence\\n                          | DIVISION division_sequence\\n            \\n            division_sequence : DIVISION unit_expression division_sequence\\n                              | DIVISION OPEN_PAREN combined_units CLOSE_PAREN division_sequence\\n                              | DIVISION product_of_units\\n                              | DIVISION OPEN_PAREN combined_units CLOSE_PAREN\\n            \\n            unit_expression : unit_with_power\\n                            | OPEN_PAREN combined_units CLOSE_PAREN\\n            \\n            factor : signed_float X UINT signed_int\\n                   | UINT X UINT signed_int\\n                   | UINT signed_int\\n                   | UINT\\n                   | signed_float\\n            \\n            unit_with_power : UNIT numeric_power\\n                            | UNIT\\n            \\n            numeric_power : sign UINT\\n            \\n            sign : SIGN\\n                 |\\n            \\n            signed_int : SIGN UINT\\n            \\n            signed_float : sign UINT\\n                         | sign UFLOAT\\n            '\n     \n-_lr_action_items = {'DIMENSIONLESS':([0,5,],[4,19,]),'OPEN_BRACKET':([0,],[5,]),'UINT':([0,10,13,16,20,21,23,31,],[7,24,-23,-24,34,35,36,40,]),'DIVISION':([0,2,5,6,7,11,14,15,16,22,24,25,26,27,30,36,39,40,41,42,],[12,12,12,-19,-18,27,-13,12,-21,-17,-26,-27,12,12,-20,-25,-14,-22,-15,-16,]),'SIGN':([0,7,16,34,35,],[13,23,13,23,23,]),'UFLOAT':([0,10,13,],[-24,25,-23,]),'OPEN_PAREN':([0,2,5,6,7,12,15,22,24,25,26,27,36,41,42,],[15,15,15,-19,-18,15,15,-17,-26,-27,15,15,-25,-15,-16,]),'UNIT':([0,2,5,6,7,12,15,22,24,25,26,27,36,41,42,],[16,16,16,-19,-18,16,16,-17,-26,-27,16,16,-25,-15,-16,]),'$end':([1,2,3,4,6,7,8,9,11,14,16,17,22,24,25,28,30,32,33,36,37,38,39,40,41,42,],[0,-6,-2,-3,-19,-18,-7,-8,-10,-13,-21,-1,-17,-26,-27,-11,-20,-4,-5,-25,-9,-12,-14,-22,-15,-16,]),'X':([6,7,24,25,],[20,21,-26,-27,]),'CLOSE_BRACKET':([8,9,11,14,16,18,19,28,30,37,38,39,40,],[-7,-8,-10,-13,-21,32,33,-11,-20,-9,-12,-14,-22,]),'CLOSE_PAREN':([8,9,11,14,16,28,29,30,37,38,39,40,],[-7,-8,-10,-13,-21,-11,39,-20,-9,-12,-14,-22,]),'PRODUCT':([11,14,16,30,39,40,],[26,-13,-21,-20,-14,-22,]),}\n+_lr_action_items = {'DIMENSIONLESS':([0,5,],[4,19,]),'OPEN_BRACKET':([0,],[5,]),'UINT':([0,10,14,16,20,21,23,32,],[7,24,-27,-28,35,36,37,45,]),'DIVISION':([0,2,5,6,7,11,12,13,15,16,22,24,25,31,37,38,39,41,44,45,46,47,50,],[12,12,12,-23,-22,27,27,-17,12,-25,-21,-30,-31,-24,-29,27,12,-17,-18,-26,-19,-20,27,]),'SIGN':([0,7,16,35,36,],[14,23,14,23,23,]),'UFLOAT':([0,10,14,],[-28,25,-27,]),'OPEN_PAREN':([0,2,5,6,7,15,22,24,25,27,37,39,46,47,],[15,15,15,-23,-22,15,-21,-30,-31,39,-29,15,-19,-20,]),'UNIT':([0,2,5,6,7,15,22,24,25,27,29,37,39,46,47,],[16,16,16,-23,-22,16,-21,-30,-31,16,16,-29,16,-19,-20,]),'$end':([1,2,3,4,6,7,8,9,13,16,17,22,24,25,26,28,31,33,34,37,40,41,42,43,45,46,47,48,50,51,],[0,-6,-2,-3,-23,-22,-7,-8,-10,-25,-1,-21,-30,-31,-11,-12,-24,-4,-5,-29,-15,-10,-10,-9,-26,-19,-20,-13,-16,-14,]),'X':([6,7,24,25,],[20,21,-30,-31,]),'CLOSE_BRACKET':([8,9,13,16,18,19,26,28,31,40,41,42,43,45,48,50,51,],[-7,-8,-10,-25,33,34,-11,-12,-24,-15,-10,-10,-9,-26,-13,-16,-14,]),'CLOSE_PAREN':([8,9,13,16,26,28,30,31,40,41,42,43,45,48,49,50,51,],[-7,-8,-10,-25,-11,-12,44,-24,-15,-10,-10,-9,-26,-13,50,-16,-14,]),'PRODUCT':([13,16,31,41,42,45,],[29,-25,-24,29,29,-26,]),}\n \n _lr_action = {}\n for _k, _v in _lr_action_items.items():\n@@ -28,7 +28,7 @@ for _k, _v in _lr_action_items.items():\n       _lr_action[_x][_k] = _y\n del _lr_action_items\n \n-_lr_goto_items = {'main':([0,],[1,]),'factor':([0,],[2,]),'combined_units':([0,2,5,15,26,27,],[3,17,18,29,37,38,]),'signed_float':([0,],[6,]),'product_of_units':([0,2,5,15,26,27,],[8,8,8,8,8,8,]),'division_of_units':([0,2,5,15,26,27,],[9,9,9,9,9,9,]),'sign':([0,16,],[10,31,]),'unit_expression':([0,2,5,12,15,26,27,],[11,11,11,28,11,11,11,]),'unit_with_power':([0,2,5,12,15,26,27,],[14,14,14,14,14,14,14,]),'signed_int':([7,34,35,],[22,41,42,]),'numeric_power':([16,],[30,]),}\n+_lr_goto_items = {'main':([0,],[1,]),'factor':([0,],[2,]),'combined_units':([0,2,5,15,39,],[3,17,18,30,49,]),'signed_float':([0,],[6,]),'division_list':([0,2,5,15,39,],[8,8,8,8,8,]),'product_of_units':([0,2,5,15,27,29,39,],[9,9,9,9,40,43,9,]),'sign':([0,16,],[10,32,]),'unit_expression':([0,2,5,15,27,39,],[11,11,11,11,38,11,]),'unit_with_power':([0,2,5,15,27,29,39,],[13,13,13,13,41,42,13,]),'signed_int':([7,35,36,],[22,46,47,]),'division_sequence':([11,12,38,50,],[26,28,48,51,]),'numeric_power':([16,],[31,]),}\n \n _lr_goto = {}\n for _k, _v in _lr_goto_items.items():\n@@ -38,31 +38,35 @@ for _k, _v in _lr_goto_items.items():\n del _lr_goto_items\n _lr_productions = [\n   (\"S' -> main\",\"S'\",1,None,None,None),\n-  ('main -> factor combined_units','main',2,'p_main','cds.py',156),\n-  ('main -> combined_units','main',1,'p_main','cds.py',157),\n-  ('main -> DIMENSIONLESS','main',1,'p_main','cds.py',158),\n-  ('main -> OPEN_BRACKET combined_units CLOSE_BRACKET','main',3,'p_main','cds.py',159),\n-  ('main -> OPEN_BRACKET DIMENSIONLESS CLOSE_BRACKET','main',3,'p_main','cds.py',160),\n-  ('main -> factor','main',1,'p_main','cds.py',161),\n-  ('combined_units -> product_of_units','combined_units',1,'p_combined_units','cds.py',174),\n-  ('combined_units -> division_of_units','combined_units',1,'p_combined_units','cds.py',175),\n-  ('product_of_units -> unit_expression PRODUCT combined_units','product_of_units',3,'p_product_of_units','cds.py',181),\n-  ('product_of_units -> unit_expression','product_of_units',1,'p_product_of_units','cds.py',182),\n-  ('division_of_units -> DIVISION unit_expression','division_of_units',2,'p_division_of_units','cds.py',191),\n-  ('division_of_units -> unit_expression DIVISION combined_units','division_of_units',3,'p_division_of_units','cds.py',192),\n-  ('unit_expression -> unit_with_power','unit_expression',1,'p_unit_expression','cds.py',201),\n-  ('unit_expression -> OPEN_PAREN combined_units CLOSE_PAREN','unit_expression',3,'p_unit_expression','cds.py',202),\n-  ('factor -> signed_float X UINT signed_int','factor',4,'p_factor','cds.py',211),\n-  ('factor -> UINT X UINT signed_int','factor',4,'p_factor','cds.py',212),\n-  ('factor -> UINT signed_int','factor',2,'p_factor','cds.py',213),\n-  ('factor -> UINT','factor',1,'p_factor','cds.py',214),\n-  ('factor -> signed_float','factor',1,'p_factor','cds.py',215),\n-  ('unit_with_power -> UNIT numeric_power','unit_with_power',2,'p_unit_with_power','cds.py',232),\n-  ('unit_with_power -> UNIT','unit_with_power',1,'p_unit_with_power','cds.py',233),\n-  ('numeric_power -> sign UINT','numeric_power',2,'p_numeric_power','cds.py',242),\n-  ('sign -> SIGN','sign',1,'p_sign','cds.py',248),\n-  ('sign -> <empty>','sign',0,'p_sign','cds.py',249),\n-  ('signed_int -> SIGN UINT','signed_int',2,'p_signed_int','cds.py',258),\n-  ('signed_float -> sign UINT','signed_float',2,'p_signed_float','cds.py',264),\n-  ('signed_float -> sign UFLOAT','signed_float',2,'p_signed_float','cds.py',265),\n+  ('main -> factor combined_units','main',2,'p_main','cds.py',148),\n+  ('main -> combined_units','main',1,'p_main','cds.py',149),\n+  ('main -> DIMENSIONLESS','main',1,'p_main','cds.py',150),\n+  ('main -> OPEN_BRACKET combined_units CLOSE_BRACKET','main',3,'p_main','cds.py',151),\n+  ('main -> OPEN_BRACKET DIMENSIONLESS CLOSE_BRACKET','main',3,'p_main','cds.py',152),\n+  ('main -> factor','main',1,'p_main','cds.py',153),\n+  ('combined_units -> division_list','combined_units',1,'p_combined_units','cds.py',167),\n+  ('combined_units -> product_of_units','combined_units',1,'p_combined_units','cds.py',168),\n+  ('product_of_units -> unit_with_power PRODUCT product_of_units','product_of_units',3,'p_product_of_units','cds.py',174),\n+  ('product_of_units -> unit_with_power','product_of_units',1,'p_product_of_units','cds.py',175),\n+  ('division_list -> unit_expression division_sequence','division_list',2,'p_division_list','cds.py',184),\n+  ('division_list -> DIVISION division_sequence','division_list',2,'p_division_list','cds.py',185),\n+  ('division_sequence -> DIVISION unit_expression division_sequence','division_sequence',3,'p_division_sequence','cds.py',200),\n+  ('division_sequence -> DIVISION OPEN_PAREN combined_units CLOSE_PAREN division_sequence','division_sequence',5,'p_division_sequence','cds.py',201),\n+  ('division_sequence -> DIVISION product_of_units','division_sequence',2,'p_division_sequence','cds.py',202),\n+  ('division_sequence -> DIVISION OPEN_PAREN combined_units CLOSE_PAREN','division_sequence',4,'p_division_sequence','cds.py',203),\n+  ('unit_expression -> unit_with_power','unit_expression',1,'p_unit_expression','cds.py',220),\n+  ('unit_expression -> OPEN_PAREN combined_units CLOSE_PAREN','unit_expression',3,'p_unit_expression','cds.py',221),\n+  ('factor -> signed_float X UINT signed_int','factor',4,'p_factor','cds.py',230),\n+  ('factor -> UINT X UINT signed_int','factor',4,'p_factor','cds.py',231),\n+  ('factor -> UINT signed_int','factor',2,'p_factor','cds.py',232),\n+  ('factor -> UINT','factor',1,'p_factor','cds.py',233),\n+  ('factor -> signed_float','factor',1,'p_factor','cds.py',234),\n+  ('unit_with_power -> UNIT numeric_power','unit_with_power',2,'p_unit_with_power','cds.py',249),\n+  ('unit_with_power -> UNIT','unit_with_power',1,'p_unit_with_power','cds.py',250),\n+  ('numeric_power -> sign UINT','numeric_power',2,'p_numeric_power','cds.py',259),\n+  ('sign -> SIGN','sign',1,'p_sign','cds.py',265),\n+  ('sign -> <empty>','sign',0,'p_sign','cds.py',266),\n+  ('signed_int -> SIGN UINT','signed_int',2,'p_signed_int','cds.py',275),\n+  ('signed_float -> sign UINT','signed_float',2,'p_signed_float','cds.py',281),\n+  ('signed_float -> sign UFLOAT','signed_float',2,'p_signed_float','cds.py',282),\n ]\n"}
{"instance_id": "django__django-12325", "model_name_or_path": "devchain", "model_patch": "diff --git a/django/db/models/base.py b/django/db/models/base.py\nindex 8ea6c05ef9..41d7245e1b 100644\n--- a/django/db/models/base.py\n+++ b/django/db/models/base.py\n@@ -202,7 +202,7 @@ class ModelBase(type):\n                 continue\n             # Locate OneToOneField instances.\n             for field in base._meta.local_fields:\n-                if isinstance(field, OneToOneField):\n+                if isinstance(field, OneToOneField) and field.remote_field.parent_link:\n                     related = resolve_relation(new_class, field.remote_field.model)\n                     parent_links[make_model_tuple(related)] = field\n \n@@ -239,6 +239,17 @@ class ModelBase(type):\n                 if base_key in parent_links:\n                     field = parent_links[base_key]\n                 elif not is_proxy:\n+                    # Check for user-declared OneToOneFields to this base without\n+                    # parent_link=True before auto-generating a parent link.\n+                    for field in new_class._meta.local_fields:\n+                        if isinstance(field, OneToOneField):\n+                            related = resolve_relation(new_class, field.remote_field.model)\n+                            if (make_model_tuple(related) == base_key and\n+                                    not field.remote_field.parent_link and\n+                                    not field.auto_created):\n+                                raise FieldError(\n+                                    \"Add parent_link=True to %s.\" % field\n+                                )\n                     attr_name = '%s_ptr' % base._meta.model_name\n                     field = OneToOneField(\n                         base,\n"}
{"instance_id": "pylint-dev__pylint-7277", "model_name_or_path": "devchain", "model_patch": "diff --git a/pylint/__init__.py b/pylint/__init__.py\nindex 9f3dd74e6..186c1cd79 100644\n--- a/pylint/__init__.py\n+++ b/pylint/__init__.py\n@@ -86,7 +86,7 @@ def modify_sys_path() -> None:\n     stdlib or pylint's own modules.\n     CPython issue: https://bugs.python.org/issue33053\n \n-    - Remove the first entry. This will always be either \"\" or the working directory\n+    - Remove the first entry if it's either \"\" or the working directory\n     - Remove the working directory from the second and third entries\n       if PYTHONPATH includes a \":\" at the beginning or the end.\n       https://github.com/PyCQA/pylint/issues/3636\n@@ -96,7 +96,12 @@ def modify_sys_path() -> None:\n       if pylint is installed in an editable configuration (as the last item).\n       https://github.com/PyCQA/pylint/issues/4161\n     \"\"\"\n-    sys.path.pop(0)\n+    # Only remove the first entry if it's the current working directory.\n+    # When pylint is run via runpy, a custom path may have been inserted\n+    # at the beginning of sys.path, which should not be removed.\n+    # See https://github.com/PyCQA/pylint/issues/7277\n+    if sys.path[0] in (\"\", \".\", os.getcwd()):\n+        sys.path.pop(0)\n     env_pythonpath = os.environ.get(\"PYTHONPATH\", \"\")\n     cwd = os.getcwd()\n     if env_pythonpath.startswith(\":\") and env_pythonpath not in (f\":{cwd}\", \":.\"):\n"}
{"instance_id": "pylint-dev__pylint-4551", "model_name_or_path": "devchain", "model_patch": "diff --git a/pylint/pyreverse/writer.py b/pylint/pyreverse/writer.py\nindex 0d80bb9ca..5b9e7f97a 100644\n--- a/pylint/pyreverse/writer.py\n+++ b/pylint/pyreverse/writer.py\n@@ -134,11 +134,36 @@ class DotWriter(DiagramWriter):\n         if not self.config.only_classnames:\n             label = r\"{}|{}\\l|\".format(label, r\"\\l\".join(obj.attrs))\n             for func in obj.methods:\n+                args = []\n+                # Process positional arguments\n                 if func.args.args:\n-                    args = [arg.name for arg in func.args.args if arg.name != \"self\"]\n-                else:\n-                    args = []\n-                label = r\"{}{}({})\\l\".format(label, func.name, \", \".join(args))\n+                    for i, arg in enumerate(func.args.args):\n+                        if arg.name == \"self\":\n+                            continue\n+                        arg_str = arg.name\n+                        # Add type annotation if present\n+                        annotations = func.args.annotations\n+                        if i < len(annotations) and annotations[i] is not None:\n+                            arg_str = \"{}: {}\".format(arg_str, annotations[i].as_string())\n+                        args.append(arg_str)\n+                # Process *args\n+                if func.args.vararg:\n+                    arg_str = \"*{}\".format(func.args.vararg)\n+                    if func.args.varargannotation is not None:\n+                        arg_str = \"{}: {}\".format(arg_str, func.args.varargannotation.as_string())\n+                    args.append(arg_str)\n+                # Process **kwargs\n+                if func.args.kwarg:\n+                    arg_str = \"**{}\".format(func.args.kwarg)\n+                    if func.args.kwargannotation is not None:\n+                        arg_str = \"{}: {}\".format(arg_str, func.args.kwargannotation.as_string())\n+                    args.append(arg_str)\n+                # Build method signature\n+                method_str = \"{}({})\".format(func.name, \", \".join(args))\n+                # Add return type if present\n+                if func.returns is not None:\n+                    method_str = \"{} -> {}\".format(method_str, func.returns.as_string())\n+                label = r\"{}{}\\l\".format(label, method_str)\n             label = \"{%s}\" % label\n         if is_exception(obj.node):\n             return dict(fontcolor=\"red\", label=label, shape=\"record\")\n"}
{"instance_id": "psf__requests-5414", "model_name_or_path": "devchain", "model_patch": "diff --git a/requests/adapters.py b/requests/adapters.py\nindex fe22ff45..a913779e 100644\n--- a/requests/adapters.py\n+++ b/requests/adapters.py\n@@ -27,7 +27,7 @@ from urllib3.exceptions import ProtocolError\n from urllib3.exceptions import ReadTimeoutError\n from urllib3.exceptions import SSLError as _SSLError\n from urllib3.exceptions import ResponseError\n-from urllib3.exceptions import LocationValueError\n+from urllib3.exceptions import LocationValueError, LocationParseError\n \n from .models import Response\n from .compat import urlparse, basestring\n@@ -411,7 +411,7 @@ class HTTPAdapter(BaseAdapter):\n \n         try:\n             conn = self.get_connection(request.url, proxies)\n-        except LocationValueError as e:\n+        except (LocationValueError, LocationParseError) as e:\n             raise InvalidURL(e, request=request)\n \n         self.cert_verify(conn, request.url, verify, cert)\n@@ -524,6 +524,9 @@ class HTTPAdapter(BaseAdapter):\n         except _ProxyError as e:\n             raise ProxyError(e)\n \n+        except LocationParseError as e:\n+            raise InvalidURL(e, request=request)\n+\n         except (_SSLError, _HTTPError) as e:\n             if isinstance(e, _SSLError):\n                 # This branch is for urllib3 versions earlier than v1.22\n"}
{"instance_id": "django__django-12406", "model_name_or_path": "devchain", "model_patch": "diff --git a/django/forms/models.py b/django/forms/models.py\nindex dafc11f995..104aac3b27 100644\n--- a/django/forms/models.py\n+++ b/django/forms/models.py\n@@ -1185,7 +1185,7 @@ class ModelChoiceField(ChoiceField):\n                  required=True, widget=None, label=None, initial=None,\n                  help_text='', to_field_name=None, limit_choices_to=None,\n                  **kwargs):\n-        if required and (initial is not None):\n+        if required and (initial is not None or (widget is not None and getattr(widget, 'input_type', None) == 'radio')):\n             self.empty_label = None\n         else:\n             self.empty_label = empty_label\n"}
{"instance_id": "astropy__astropy-13236", "model_name_or_path": "devchain", "model_patch": "diff --git a/astropy/table/table.py b/astropy/table/table.py\nindex 5afe2127a3..9d8efb4545 100644\n--- a/astropy/table/table.py\n+++ b/astropy/table/table.py\n@@ -1243,6 +1243,12 @@ class Table:\n         # mixin class\n         if (not isinstance(data, Column) and not data_is_mixin\n                 and isinstance(data, np.ndarray) and len(data.dtype) > 1):\n+            warnings.warn(\"Structured data arrays passed to Table are automatically \"\n+                          \"converted to NdarrayMixin. In a future version of astropy, \"\n+                          \"they will be converted to Column objects instead. To avoid \"\n+                          \"this warning, explicitly wrap your data in Column, e.g. \"\n+                          \"Table([Column(data)], names=...).\",\n+                          FutureWarning, stacklevel=3)\n             data = data.view(NdarrayMixin)\n             data_is_mixin = True\n \n"}
{"instance_id": "django__django-14792", "model_name_or_path": "devchain", "model_patch": "diff --git a/django/db/backends/postgresql/operations.py b/django/db/backends/postgresql/operations.py\nindex 8d19872bea..0825b09048 100644\n--- a/django/db/backends/postgresql/operations.py\n+++ b/django/db/backends/postgresql/operations.py\n@@ -44,6 +44,12 @@ class DatabaseOperations(BaseDatabaseOperations):\n         return \"DATE_TRUNC('%s', %s)\" % (lookup_type, field_name)\n \n     def _prepare_tzname_delta(self, tzname):\n+        # Timezone names from the timezone database (like 'Etc/GMT-10')\n+        # already follow the POSIX convention where the sign is flipped\n+        # relative to the common usage. Only flip the sign for simple\n+        # offset strings like '+10' or '-10'.\n+        if tzname.startswith('Etc/GMT'):\n+            return tzname\n         if '+' in tzname:\n             return tzname.replace('+', '-')\n         elif '-' in tzname:\n"}
{"instance_id": "sympy__sympy-20916", "model_name_or_path": "devchain", "model_patch": "diff --git a/sympy/printing/conventions.py b/sympy/printing/conventions.py\nindex 9a3be6bbe9..978643cb72 100644\n--- a/sympy/printing/conventions.py\n+++ b/sympy/printing/conventions.py\n@@ -7,7 +7,7 @@\n from collections.abc import Iterable\n from sympy import Derivative\n \n-_name_with_digits_p = re.compile(r'^([a-zA-Z]+)([0-9]+)$')\n+_name_with_digits_p = re.compile(r'^([^\\W\\d_]+)([0-9]+)$', re.UNICODE)\n \n \n def split_super_sub(text):\n"}
{"instance_id": "django__django-14011", "model_name_or_path": "devchain", "model_patch": "diff --git a/django/core/servers/basehttp.py b/django/core/servers/basehttp.py\nindex 14dab517c9..6cc8a46778 100644\n--- a/django/core/servers/basehttp.py\n+++ b/django/core/servers/basehttp.py\n@@ -16,6 +16,7 @@ from wsgiref import simple_server\n from django.core.exceptions import ImproperlyConfigured\n from django.core.handlers.wsgi import LimitedStream\n from django.core.wsgi import get_wsgi_application\n+from django.db import connections\n from django.utils.module_loading import import_string\n \n __all__ = ('WSGIServer', 'WSGIRequestHandler')\n@@ -81,6 +82,28 @@ class ThreadedWSGIServer(socketserver.ThreadingMixIn, WSGIServer):\n     \"\"\"A threaded version of the WSGIServer\"\"\"\n     daemon_threads = True\n \n+    def __init__(self, *args, connections_override=None, **kwargs):\n+        super().__init__(*args, **kwargs)\n+        self.connections_override = connections_override\n+\n+    # socketserver.ThreadingMixIn.process_request() passes this method as\n+    # the target to a new Thread object.\n+    def process_request_thread(self, request, client_address):\n+        if self.connections_override:\n+            # Override this thread's database connections with the ones\n+            # provided by the parent thread.\n+            for alias, conn in self.connections_override.items():\n+                connections[alias] = conn\n+        super().process_request_thread(request, client_address)\n+\n+    def _close_connections(self):\n+        # Used for mocking in tests.\n+        connections.close_all()\n+\n+    def close_request(self, request):\n+        self._close_connections()\n+        super().close_request(request)\n+\n \n class ServerHandler(simple_server.ServerHandler):\n     http_version = '1.1'\ndiff --git a/django/test/testcases.py b/django/test/testcases.py\nindex 92f566b7b0..991fa72dff 100644\n--- a/django/test/testcases.py\n+++ b/django/test/testcases.py\n@@ -1500,7 +1500,7 @@ class LiveServerThread(threading.Thread):\n         try:\n             # Create the handler for serving static and media files\n             handler = self.static_handler(_MediaFilesHandler(WSGIHandler()))\n-            self.httpd = self._create_server()\n+            self.httpd = self._create_server(self.connections_override)\n             # If binding to port zero, assign the port allocated by the OS.\n             if self.port == 0:\n                 self.port = self.httpd.server_address[1]\n@@ -1513,11 +1513,12 @@ class LiveServerThread(threading.Thread):\n         finally:\n             connections.close_all()\n \n-    def _create_server(self):\n+    def _create_server(self, connections_override=None):\n         return self.server_class(\n             (self.host, self.port),\n             QuietWSGIRequestHandler,\n             allow_reuse_address=False,\n+            connections_override=connections_override,\n         )\n \n     def terminate(self):\n@@ -1593,7 +1594,7 @@ class LiveServerTestCase(TransactionTestCase):\n     def _tearDownClassInternal(cls):\n         # Terminate the live server's thread.\n         cls.server_thread.terminate()\n-        # Restore sqlite in-memory database connections' non-shareability.\n+        # Restore shared connections' non-shareability.\n         for conn in cls.server_thread.connections_override.values():\n             conn.dec_thread_sharing()\n \n"}
{"instance_id": "django__django-16560", "model_name_or_path": "devchain", "model_patch": "diff --git a/django/contrib/postgres/constraints.py b/django/contrib/postgres/constraints.py\nindex ad3a5f61f5..058bb01488 100644\n--- a/django/contrib/postgres/constraints.py\n+++ b/django/contrib/postgres/constraints.py\n@@ -33,6 +33,7 @@ class ExclusionConstraint(BaseConstraint):\n         deferrable=None,\n         include=None,\n         violation_error_message=None,\n+        violation_error_code=None,\n     ):\n         if index_type and index_type.lower() not in {\"gist\", \"spgist\"}:\n             raise ValueError(\n@@ -60,7 +61,11 @@ class ExclusionConstraint(BaseConstraint):\n         self.condition = condition\n         self.deferrable = deferrable\n         self.include = tuple(include) if include else ()\n-        super().__init__(name=name, violation_error_message=violation_error_message)\n+        super().__init__(\n+            name=name,\n+            violation_error_message=violation_error_message,\n+            violation_error_code=violation_error_code,\n+        )\n \n     def _get_expressions(self, schema_editor, query):\n         expressions = []\n@@ -150,11 +155,12 @@ class ExclusionConstraint(BaseConstraint):\n                 and self.deferrable == other.deferrable\n                 and self.include == other.include\n                 and self.violation_error_message == other.violation_error_message\n+                and self.violation_error_code == other.violation_error_code\n             )\n         return super().__eq__(other)\n \n     def __repr__(self):\n-        return \"<%s: index_type=%s expressions=%s name=%s%s%s%s%s>\" % (\n+        return \"<%s: index_type=%s expressions=%s name=%s%s%s%s%s%s>\" % (\n             self.__class__.__qualname__,\n             repr(self.index_type),\n             repr(self.expressions),\n@@ -168,6 +174,11 @@ class ExclusionConstraint(BaseConstraint):\n                 or self.violation_error_message == self.default_violation_error_message\n                 else \" violation_error_message=%r\" % self.violation_error_message\n             ),\n+            (\n+                \"\"\n+                if self.violation_error_code is None\n+                else \" violation_error_code=%r\" % self.violation_error_code\n+            ),\n         )\n \n     def validate(self, model, instance, exclude=None, using=DEFAULT_DB_ALIAS):\n@@ -204,9 +215,15 @@ class ExclusionConstraint(BaseConstraint):\n             queryset = queryset.exclude(pk=model_class_pk)\n         if not self.condition:\n             if queryset.exists():\n-                raise ValidationError(self.get_violation_error_message())\n+                raise ValidationError(\n+                    self.get_violation_error_message(),\n+                    code=self.get_violation_error_code(),\n+                )\n         else:\n             if (self.condition & Exists(queryset.filter(self.condition))).check(\n                 replacement_map, using=using\n             ):\n-                raise ValidationError(self.get_violation_error_message())\n+                raise ValidationError(\n+                    self.get_violation_error_message(),\n+                    code=self.get_violation_error_code(),\n+                )\ndiff --git a/django/db/models/constraints.py b/django/db/models/constraints.py\nindex 075ecee1be..1494867179 100644\n--- a/django/db/models/constraints.py\n+++ b/django/db/models/constraints.py\n@@ -18,11 +18,13 @@ __all__ = [\"BaseConstraint\", \"CheckConstraint\", \"Deferrable\", \"UniqueConstraint\"\n \n class BaseConstraint:\n     default_violation_error_message = _(\"Constraint \u201c%(name)s\u201d is violated.\")\n+    default_violation_error_code = None\n     violation_error_message = None\n+    violation_error_code = None\n \n     # RemovedInDjango60Warning: When the deprecation ends, replace with:\n-    # def __init__(self, *, name, violation_error_message=None):\n-    def __init__(self, *args, name=None, violation_error_message=None):\n+    # def __init__(self, *, name, violation_error_message=None, violation_error_code=None):\n+    def __init__(self, *args, name=None, violation_error_message=None, violation_error_code=None):\n         # RemovedInDjango60Warning.\n         if name is None and not args:\n             raise TypeError(\n@@ -34,6 +36,10 @@ class BaseConstraint:\n             self.violation_error_message = violation_error_message\n         else:\n             self.violation_error_message = self.default_violation_error_message\n+        if violation_error_code is not None:\n+            self.violation_error_code = violation_error_code\n+        else:\n+            self.violation_error_code = self.default_violation_error_code\n         # RemovedInDjango60Warning.\n         if args:\n             warnings.warn(\n@@ -42,7 +48,7 @@ class BaseConstraint:\n                 RemovedInDjango60Warning,\n                 stacklevel=2,\n             )\n-            for arg, attr in zip(args, [\"name\", \"violation_error_message\"]):\n+            for arg, attr in zip(args, [\"name\", \"violation_error_message\", \"violation_error_code\"]):\n                 if arg:\n                     setattr(self, attr, arg)\n \n@@ -65,6 +71,9 @@ class BaseConstraint:\n     def get_violation_error_message(self):\n         return self.violation_error_message % {\"name\": self.name}\n \n+    def get_violation_error_code(self):\n+        return self.violation_error_code\n+\n     def deconstruct(self):\n         path = \"%s.%s\" % (self.__class__.__module__, self.__class__.__name__)\n         path = path.replace(\"django.db.models.constraints\", \"django.db.models\")\n@@ -74,6 +83,8 @@ class BaseConstraint:\n             and self.violation_error_message != self.default_violation_error_message\n         ):\n             kwargs[\"violation_error_message\"] = self.violation_error_message\n+        if self.violation_error_code is not None:\n+            kwargs[\"violation_error_code\"] = self.violation_error_code\n         return (path, (), kwargs)\n \n     def clone(self):\n@@ -82,13 +93,17 @@ class BaseConstraint:\n \n \n class CheckConstraint(BaseConstraint):\n-    def __init__(self, *, check, name, violation_error_message=None):\n+    def __init__(self, *, check, name, violation_error_message=None, violation_error_code=None):\n         self.check = check\n         if not getattr(check, \"conditional\", False):\n             raise TypeError(\n                 \"CheckConstraint.check must be a Q instance or boolean expression.\"\n             )\n-        super().__init__(name=name, violation_error_message=violation_error_message)\n+        super().__init__(\n+            name=name,\n+            violation_error_message=violation_error_message,\n+            violation_error_code=violation_error_code,\n+        )\n \n     def _get_check_sql(self, model, schema_editor):\n         query = Query(model=model, alias_cols=False)\n@@ -112,12 +127,15 @@ class CheckConstraint(BaseConstraint):\n         against = instance._get_field_value_map(meta=model._meta, exclude=exclude)\n         try:\n             if not Q(self.check).check(against, using=using):\n-                raise ValidationError(self.get_violation_error_message())\n+                raise ValidationError(\n+                    self.get_violation_error_message(),\n+                    code=self.get_violation_error_code(),\n+                )\n         except FieldError:\n             pass\n \n     def __repr__(self):\n-        return \"<%s: check=%s name=%s%s>\" % (\n+        return \"<%s: check=%s name=%s%s%s>\" % (\n             self.__class__.__qualname__,\n             self.check,\n             repr(self.name),\n@@ -127,6 +145,11 @@ class CheckConstraint(BaseConstraint):\n                 or self.violation_error_message == self.default_violation_error_message\n                 else \" violation_error_message=%r\" % self.violation_error_message\n             ),\n+            (\n+                \"\"\n+                if self.violation_error_code is None\n+                else \" violation_error_code=%r\" % self.violation_error_code\n+            ),\n         )\n \n     def __eq__(self, other):\n@@ -135,6 +158,7 @@ class CheckConstraint(BaseConstraint):\n                 self.name == other.name\n                 and self.check == other.check\n                 and self.violation_error_message == other.violation_error_message\n+                and self.violation_error_code == other.violation_error_code\n             )\n         return super().__eq__(other)\n \n@@ -164,6 +188,7 @@ class UniqueConstraint(BaseConstraint):\n         include=None,\n         opclasses=(),\n         violation_error_message=None,\n+        violation_error_code=None,\n     ):\n         if not name:\n             raise ValueError(\"A unique constraint must be named.\")\n@@ -213,7 +238,11 @@ class UniqueConstraint(BaseConstraint):\n             F(expression) if isinstance(expression, str) else expression\n             for expression in expressions\n         )\n-        super().__init__(name=name, violation_error_message=violation_error_message)\n+        super().__init__(\n+            name=name,\n+            violation_error_message=violation_error_message,\n+            violation_error_code=violation_error_code,\n+        )\n \n     @property\n     def contains_expressions(self):\n@@ -293,7 +322,7 @@ class UniqueConstraint(BaseConstraint):\n         )\n \n     def __repr__(self):\n-        return \"<%s:%s%s%s%s%s%s%s%s>\" % (\n+        return \"<%s:%s%s%s%s%s%s%s%s%s>\" % (\n             self.__class__.__qualname__,\n             \"\" if not self.fields else \" fields=%s\" % repr(self.fields),\n             \"\" if not self.expressions else \" expressions=%s\" % repr(self.expressions),\n@@ -308,6 +337,11 @@ class UniqueConstraint(BaseConstraint):\n                 or self.violation_error_message == self.default_violation_error_message\n                 else \" violation_error_message=%r\" % self.violation_error_message\n             ),\n+            (\n+                \"\"\n+                if self.violation_error_code is None\n+                else \" violation_error_code=%r\" % self.violation_error_code\n+            ),\n         )\n \n     def __eq__(self, other):\n@@ -321,6 +355,7 @@ class UniqueConstraint(BaseConstraint):\n                 and self.opclasses == other.opclasses\n                 and self.expressions == other.expressions\n                 and self.violation_error_message == other.violation_error_message\n+                and self.violation_error_code == other.violation_error_code\n             )\n         return super().__eq__(other)\n \n@@ -385,7 +420,10 @@ class UniqueConstraint(BaseConstraint):\n         if not self.condition:\n             if queryset.exists():\n                 if self.expressions:\n-                    raise ValidationError(self.get_violation_error_message())\n+                    raise ValidationError(\n+                        self.get_violation_error_message(),\n+                        code=self.get_violation_error_code(),\n+                    )\n                 # When fields are defined, use the unique_error_message() for\n                 # backward compatibility.\n                 for model, constraints in instance.get_constraints():\n@@ -400,6 +438,9 @@ class UniqueConstraint(BaseConstraint):\n                 if (self.condition & Exists(queryset.filter(self.condition))).check(\n                     against, using=using\n                 ):\n-                    raise ValidationError(self.get_violation_error_message())\n+                    raise ValidationError(\n+                        self.get_violation_error_message(),\n+                        code=self.get_violation_error_code(),\n+                    )\n             except FieldError:\n                 pass\n"}
{"instance_id": "django__django-15629", "model_name_or_path": "devchain", "model_patch": "diff --git a/django/db/models/fields/related.py b/django/db/models/fields/related.py\nindex fad5560486..201aa8abe6 100644\n--- a/django/db/models/fields/related.py\n+++ b/django/db/models/fields/related.py\n@@ -1180,7 +1180,12 @@ class ForeignKey(ForeignObject):\n         return self.target_field.rel_db_type(connection=connection)\n \n     def db_parameters(self, connection):\n-        return {\"type\": self.db_type(connection), \"check\": self.db_check(connection)}\n+        db_params = {\"type\": self.db_type(connection), \"check\": self.db_check(connection)}\n+        # Propagate db_collation from the target field to ensure that foreign key\n+        # columns have the same collation as the referenced primary key column.\n+        if hasattr(self.target_field, \"db_collation\"):\n+            db_params[\"collation\"] = self.target_field.db_collation\n+        return db_params\n \n     def convert_empty_strings(self, value, expression, connection):\n         if (not value) and isinstance(value, str):\n"}
{"instance_id": "scikit-learn__scikit-learn-25747", "model_name_or_path": "devchain", "model_patch": "diff --git a/sklearn/utils/_set_output.py b/sklearn/utils/_set_output.py\nindex 335773c6a..5652c3aab 100644\n--- a/sklearn/utils/_set_output.py\n+++ b/sklearn/utils/_set_output.py\n@@ -55,10 +55,23 @@ def _wrap_in_pandas_container(\n     if isinstance(data_to_wrap, pd.DataFrame):\n         if columns is not None:\n             data_to_wrap.columns = columns\n-        if index is not None:\n+        if index is not None and len(data_to_wrap.index) == len(index):\n             data_to_wrap.index = index\n         return data_to_wrap\n \n+    if isinstance(data_to_wrap, pd.Series):\n+        # For Series, preserve the existing index if:\n+        # 1. The provided index has a different length, or\n+        # 2. The Series has a non-RangeIndex (meaningful index like from groupby)\n+        # Otherwise, use the provided index (standard case)\n+        if index is not None:\n+            if len(data_to_wrap.index) != len(index):\n+                # Length mismatch: preserve Series' index\n+                index = None\n+            elif not isinstance(data_to_wrap.index, pd.RangeIndex):\n+                # Series has a meaningful index: preserve it\n+                index = None\n+\n     return pd.DataFrame(data_to_wrap, index=index, columns=columns)\n \n \n"}
{"instance_id": "matplotlib__matplotlib-22865", "model_name_or_path": "devchain", "model_patch": "diff --git a/lib/matplotlib/colorbar.py b/lib/matplotlib/colorbar.py\nindex c59b0ac815..a23f0f1f00 100644\n--- a/lib/matplotlib/colorbar.py\n+++ b/lib/matplotlib/colorbar.py\n@@ -651,8 +651,13 @@ class Colorbar:\n             if not self.drawedges:\n                 if len(self._y) >= self.n_rasterize:\n                     self.solids.set_rasterized(True)\n+        # Determine which divider segments to skip based on extend settings.\n+        # When an extension is present, draw the edge at the boundary\n+        # between the colorbar and the extension.\n+        slice_start = 1 if not self._extend_lower() else 0\n+        slice_end = -1 if not self._extend_upper() else None\n         self.dividers.set_segments(\n-            np.dstack([X, Y])[1:-1] if self.drawedges else [])\n+            np.dstack([X, Y])[slice_start:slice_end] if self.drawedges else [])\n \n     def _add_solids_patches(self, X, Y, C, mappable):\n         hatches = mappable.hatches * len(C)  # Have enough hatches.\n"}
{"instance_id": "sympy__sympy-14976", "model_name_or_path": "devchain", "model_patch": "diff --git a/sympy/printing/pycode.py b/sympy/printing/pycode.py\nindex d06901425d..a9f7ca9b94 100644\n--- a/sympy/printing/pycode.py\n+++ b/sympy/printing/pycode.py\n@@ -331,6 +331,15 @@ def _print_Float(self, e):\n         args = str(tuple(map(int, e._mpf_)))\n         return '{func}({args})'.format(func=self._module_format('mpmath.mpf'), args=args)\n \n+    def _print_Rational(self, e):\n+        # Wrap rationals with mpmath.mpf to maintain full precision.\n+        # Use mpmath.mpf for both numerator and denominator to avoid\n+        # losing precision through Python float division.\n+        return '{func}({p})/{func}({q})'.format(\n+            func=self._module_format('mpmath.mpf'),\n+            p=self._print(e.p),\n+            q=self._print(e.q))\n+\n \n     def _print_uppergamma(self, e):\n         return \"{0}({1}, {2}, {3})\".format(\n"}
{"instance_id": "django__django-14311", "model_name_or_path": "devchain", "model_patch": "diff --git a/django/utils/autoreload.py b/django/utils/autoreload.py\nindex b6af5f5491..490e0a0a9f 100644\n--- a/django/utils/autoreload.py\n+++ b/django/utils/autoreload.py\n@@ -224,7 +224,13 @@ def get_child_arguments():\n     # see https://docs.python.org/3/reference/import.html#main-spec\n     # __spec__ may not exist, e.g. when running in a Conda env.\n     if getattr(__main__, '__spec__', None) is not None and __main__.__spec__.parent:\n-        args += ['-m', __main__.__spec__.parent]\n+        # For packages (e.g. python -m django), __spec__.name ends with '.__main__',\n+        # so use __spec__.parent. For modules (e.g. python -m foo.bar.baz),\n+        # __spec__.name is the module name itself.\n+        if __main__.__spec__.name.endswith('.__main__'):\n+            args += ['-m', __main__.__spec__.parent]\n+        else:\n+            args += ['-m', __main__.__spec__.name]\n         args += sys.argv[1:]\n     elif not py_script.exists():\n         # sys.argv[0] may not exist for several reasons on Windows.\n"}
{"instance_id": "django__django-14376", "model_name_or_path": "devchain", "model_patch": "diff --git a/django/db/backends/mysql/base.py b/django/db/backends/mysql/base.py\nindex 2c62182d31..8b28a60485 100644\n--- a/django/db/backends/mysql/base.py\n+++ b/django/db/backends/mysql/base.py\n@@ -200,9 +200,9 @@ class DatabaseWrapper(BaseDatabaseWrapper):\n         if settings_dict['USER']:\n             kwargs['user'] = settings_dict['USER']\n         if settings_dict['NAME']:\n-            kwargs['db'] = settings_dict['NAME']\n+            kwargs['database'] = settings_dict['NAME']\n         if settings_dict['PASSWORD']:\n-            kwargs['passwd'] = settings_dict['PASSWORD']\n+            kwargs['password'] = settings_dict['PASSWORD']\n         if settings_dict['HOST'].startswith('/'):\n             kwargs['unix_socket'] = settings_dict['HOST']\n         elif settings_dict['HOST']:\n"}
{"instance_id": "sympy__sympy-18199", "model_name_or_path": "devchain", "model_patch": "diff --git a/sympy/ntheory/residue_ntheory.py b/sympy/ntheory/residue_ntheory.py\nindex 9a3052d4c3..01e26c9dd6 100644\n--- a/sympy/ntheory/residue_ntheory.py\n+++ b/sympy/ntheory/residue_ntheory.py\n@@ -776,6 +776,11 @@ def nthroot_mod(a, n, p, all_roots=False):\n     if not isprime(p):\n         raise NotImplementedError(\"Not implemented for composite p\")\n \n+    a_mod_p = a % p\n+    if a_mod_p == 0:\n+        # When a % p == 0, x = 0 is always a root of x**n = a mod p\n+        return 0 if not all_roots else [0]\n+\n     if (p - 1) % n == 0:\n         return _nthroot_mod1(a, n, p, all_roots)\n     # The roots of ``x**n - a = 0 (mod p)`` are roots of\n"}
{"instance_id": "django__django-16631", "model_name_or_path": "devchain", "model_patch": "diff --git a/django/contrib/auth/__init__.py b/django/contrib/auth/__init__.py\nindex 155330c596..890ade94b9 100644\n--- a/django/contrib/auth/__init__.py\n+++ b/django/contrib/auth/__init__.py\n@@ -5,7 +5,7 @@ from django.apps import apps as django_apps\n from django.conf import settings\n from django.core.exceptions import ImproperlyConfigured, PermissionDenied\n from django.middleware.csrf import rotate_token\n-from django.utils.crypto import constant_time_compare\n+from django.utils.crypto import compare_digest_with_hmac, constant_time_compare\n from django.utils.module_loading import import_string\n from django.views.decorators.debug import sensitive_variables\n \n@@ -199,10 +199,19 @@ def get_user(request):\n             # Verify the session\n             if hasattr(user, \"get_session_auth_hash\"):\n                 session_hash = request.session.get(HASH_SESSION_KEY)\n-                session_hash_verified = session_hash and constant_time_compare(\n-                    session_hash, user.get_session_auth_hash()\n+                session_hash_verified = session_hash and compare_digest_with_hmac(\n+                    session_hash,\n+                    user.password,\n+                    \"django.contrib.auth.models.AbstractBaseUser.get_session_auth_hash\",\n+                    algorithm=\"sha256\",\n                 )\n-                if not session_hash_verified:\n+                if session_hash_verified:\n+                    # If the session was verified with a fallback key, update the\n+                    # session hash to use the current key.\n+                    current_hash = user.get_session_auth_hash()\n+                    if session_hash != current_hash:\n+                        request.session[HASH_SESSION_KEY] = current_hash\n+                else:\n                     request.session.flush()\n                     user = None\n \ndiff --git a/django/utils/crypto.py b/django/utils/crypto.py\nindex 1c0e7001c6..4b0ed527e8 100644\n--- a/django/utils/crypto.py\n+++ b/django/utils/crypto.py\n@@ -66,6 +66,53 @@ def constant_time_compare(val1, val2):\n     return secrets.compare_digest(force_bytes(val1), force_bytes(val2))\n \n \n+def compare_digest_with_hmac(\n+    value_1,\n+    value_2,\n+    key_salt,\n+    *,\n+    secret=None,\n+    fallback_secrets=None,\n+    algorithm=\"sha1\",\n+):\n+    \"\"\"\n+    Compare two HMAC digest values. The value_1 is expected to be a digest\n+    (hexdigest string) and value_2 is the input to HMAC. The comparison will\n+    be tried with the primary secret and optionally with fallback secrets.\n+\n+    This is useful for verifying HMACs created with a rotated secret key.\n+\n+    Args:\n+        value_1: The stored HMAC digest (hexdigest string)\n+        value_2: The value to hash and compare\n+        key_salt: The salt for the HMAC\n+        secret: The primary secret (defaults to settings.SECRET_KEY)\n+        fallback_secrets: List of fallback secrets to try (defaults to settings.SECRET_KEY_FALLBACKS)\n+        algorithm: Hash algorithm to use (defaults to \"sha1\")\n+\n+    Returns:\n+        True if value_1 matches the HMAC of value_2 with any of the secrets, False otherwise.\n+    \"\"\"\n+    from django.conf import settings\n+\n+    if secret is None:\n+        secret = settings.SECRET_KEY\n+    if fallback_secrets is None:\n+        fallback_secrets = settings.SECRET_KEY_FALLBACKS\n+\n+    # Try the primary secret first, then fallbacks\n+    for test_secret in [secret, *fallback_secrets]:\n+        hmac_value = salted_hmac(\n+            key_salt,\n+            value_2,\n+            secret=test_secret,\n+            algorithm=algorithm,\n+        ).hexdigest()\n+        if constant_time_compare(value_1, hmac_value):\n+            return True\n+    return False\n+\n+\n def pbkdf2(password, salt, iterations, dklen=0, digest=None):\n     \"\"\"Return the hash of password using pbkdf2.\"\"\"\n     if digest is None:\n"}
{"instance_id": "django__django-15695", "model_name_or_path": "devchain", "model_patch": "diff --git a/django/db/migrations/operations/models.py b/django/db/migrations/operations/models.py\nindex d17232e4ec..05695f189d 100644\n--- a/django/db/migrations/operations/models.py\n+++ b/django/db/migrations/operations/models.py\n@@ -967,7 +967,28 @@ class RenameIndex(IndexOperation):\n \n     def database_backwards(self, app_label, schema_editor, from_state, to_state):\n         if self.old_fields:\n-            # Backward operation with unnamed index is a no-op.\n+            # Restore the old auto-generated name for index_together.\n+            to_model = to_state.apps.get_model(app_label, self.model_name)\n+            if not self.allow_migrate_model(schema_editor.connection.alias, to_model):\n+                return\n+            # Compute the auto-generated index name for index_together.\n+            columns = [\n+                to_model._meta.get_field(field).column for field in self.old_fields\n+            ]\n+            auto_generated_name = schema_editor._create_index_name(\n+                to_model._meta.db_table, columns, \"_idx\"\n+            )\n+            if auto_generated_name != self.new_name:\n+                # Only rename if the auto-generated name is different from new_name.\n+                old_index = models.Index(\n+                    fields=self.old_fields,\n+                    name=self.new_name,\n+                )\n+                new_index = models.Index(\n+                    fields=self.old_fields,\n+                    name=auto_generated_name,\n+                )\n+                schema_editor.rename_index(to_model, old_index, new_index)\n             return\n \n         self.new_name_lower, self.old_name_lower = (\n"}
{"instance_id": "sympy__sympy-20438", "model_name_or_path": "devchain", "model_patch": "diff --git a/sympy/sets/handlers/issubset.py b/sympy/sets/handlers/issubset.py\nindex fdca06025a..2bf838f49e 100644\n--- a/sympy/sets/handlers/issubset.py\n+++ b/sympy/sets/handlers/issubset.py\n@@ -1,7 +1,7 @@\n from sympy import S, Symbol\n from sympy.core.logic import fuzzy_and, fuzzy_bool, fuzzy_not, fuzzy_or\n from sympy.core.relational import Eq\n-from sympy.sets.sets import FiniteSet, Interval, Set, Union\n+from sympy.sets.sets import FiniteSet, Interval, ProductSet, Set, Union\n from sympy.sets.fancysets import Complexes, Reals, Range, Rationals\n from sympy.multipledispatch import dispatch\n \n@@ -133,3 +133,10 @@ def is_subset_sets(a, b): # noqa:F811\n @dispatch(Rationals, Range)  # type: ignore # noqa:F811\n def is_subset_sets(a, b): # noqa:F811\n     return False\n+\n+@dispatch(ProductSet, Set)  # type: ignore # noqa:F811\n+def is_subset_sets(a, b): # noqa:F811\n+    # Check if ProductSet a is a subset of Set b\n+    # If ProductSet is iterable (finite), we can check all elements\n+    if a.is_iterable:\n+        return fuzzy_and(b.contains(elem) for elem in a)\n"}
{"instance_id": "sympy__sympy-20428", "model_name_or_path": "devchain", "model_patch": "diff --git a/sympy/polys/densetools.py b/sympy/polys/densetools.py\nindex f057824642..931dfe80e1 100644\n--- a/sympy/polys/densetools.py\n+++ b/sympy/polys/densetools.py\n@@ -1198,6 +1198,8 @@ def dup_clear_denoms(f, K0, K1=None, convert=False):\n     if not K1.is_one(common):\n         f = dup_mul_ground(f, common, K0)\n \n+    f = dup_strip(f)\n+\n     if not convert:\n         return common, f\n     else:\n@@ -1252,6 +1254,8 @@ def dmp_clear_denoms(f, u, K0, K1=None, convert=False):\n     if not K1.is_one(common):\n         f = dmp_mul_ground(f, common, u, K0)\n \n+    f = dmp_strip(f, u)\n+\n     if not convert:\n         return common, f\n     else:\n"}
{"instance_id": "django__django-11848", "model_name_or_path": "devchain", "model_patch": "diff --git a/django/utils/http.py b/django/utils/http.py\nindex 572cfb4347..32f336e965 100644\n--- a/django/utils/http.py\n+++ b/django/utils/http.py\n@@ -176,10 +176,17 @@ def parse_http_date(date):\n     try:\n         year = int(m.group('year'))\n         if year < 100:\n-            if year < 70:\n-                year += 2000\n-            else:\n+            # RFC 7231 section 7.1.1.1: Recipients of a timestamp value in\n+            # rfc850-date format, which uses a two-digit year, MUST interpret\n+            # a timestamp that appears to be more than 50 years in the future\n+            # as representing the most recent year in the past that had the\n+            # same last two digits.\n+            current_year = datetime.datetime.utcnow().year\n+            candidate_year = 2000 + year\n+            if candidate_year - current_year > 50:\n                 year += 1900\n+            else:\n+                year = candidate_year\n         month = MONTHS.index(m.group('mon').lower()) + 1\n         day = int(m.group('day'))\n         hour = int(m.group('hour'))\n"}
{"instance_id": "django__django-11728", "model_name_or_path": "devchain", "model_patch": "diff --git a/django/contrib/admindocs/utils.py b/django/contrib/admindocs/utils.py\nindex 1ce4594501..692a02efa6 100644\n--- a/django/contrib/admindocs/utils.py\n+++ b/django/contrib/admindocs/utils.py\n@@ -167,19 +167,22 @@ def replace_named_groups(pattern):\n         # Handle nested parentheses, e.g. '^(?P<a>(x|y))/b'.\n         unmatched_open_brackets, prev_char = 1, None\n         for idx, val in enumerate(pattern[end:]):\n-            # If brackets are balanced, the end of the string for the current\n-            # named capture group pattern has been reached.\n-            if unmatched_open_brackets == 0:\n-                group_pattern_and_name.append((pattern[start:end + idx], group_name))\n-                break\n-\n             # Check for unescaped `(` and `)`. They mark the start and end of a\n             # nested group.\n             if val == '(' and prev_char != '\\\\':\n                 unmatched_open_brackets += 1\n             elif val == ')' and prev_char != '\\\\':\n                 unmatched_open_brackets -= 1\n+            # If brackets are balanced, the end of the string for the current\n+            # named capture group pattern has been reached.\n+            if unmatched_open_brackets == 0:\n+                group_pattern_and_name.append((pattern[start:end + idx + 1], group_name))\n+                break\n             prev_char = val\n+        else:\n+            # The pattern ended without finding the closing parenthesis.\n+            # This shouldn't happen with valid regex patterns, but handle it.\n+            group_pattern_and_name.append((pattern[start:], group_name))\n \n     # Replace the string for named capture groups with their group names.\n     for group_pattern, group_name in group_pattern_and_name:\n@@ -201,17 +204,20 @@ def replace_unnamed_groups(pattern):\n         # Handle nested parentheses, e.g. '^b/((x|y)\\w+)$'.\n         unmatched_open_brackets, prev_char = 1, None\n         for idx, val in enumerate(pattern[start + 1:]):\n-            if unmatched_open_brackets == 0:\n-                group_indices.append((start, start + 1 + idx))\n-                break\n-\n             # Check for unescaped `(` and `)`. They mark the start and end of\n             # a nested group.\n             if val == '(' and prev_char != '\\\\':\n                 unmatched_open_brackets += 1\n             elif val == ')' and prev_char != '\\\\':\n                 unmatched_open_brackets -= 1\n+            if unmatched_open_brackets == 0:\n+                group_indices.append((start, start + 1 + idx + 1))\n+                break\n             prev_char = val\n+        else:\n+            # The pattern ended without finding the closing parenthesis.\n+            # This shouldn't happen with valid regex patterns, but handle it.\n+            group_indices.append((start, len(pattern)))\n \n     # Remove unnamed group matches inside other unnamed capture groups.\n     group_start_end_indices = []\n"}
{"instance_id": "django__django-14140", "model_name_or_path": "devchain", "model_patch": "diff --git a/django/db/models/query_utils.py b/django/db/models/query_utils.py\nindex ae0f886107..482e23d00b 100644\n--- a/django/db/models/query_utils.py\n+++ b/django/db/models/query_utils.py\n@@ -87,7 +87,12 @@ class Q(tree.Node):\n         args, kwargs = (), {}\n         if len(self.children) == 1 and not isinstance(self.children[0], Q):\n             child = self.children[0]\n-            kwargs = {child[0]: child[1]}\n+            # Only use kwargs if child is a tuple with exactly 2 elements (key, value).\n+            # For non-tuple children (like Exists), use args instead.\n+            if isinstance(child, tuple) and len(child) == 2:\n+                kwargs = {child[0]: child[1]}\n+            else:\n+                args = (child,)\n         else:\n             args = tuple(self.children)\n             if self.connector != self.default:\n"}
{"instance_id": "sphinx-doc__sphinx-7440", "model_name_or_path": "devchain", "model_patch": "diff --git a/sphinx/domains/std.py b/sphinx/domains/std.py\nindex d820cfe5c..ff1cec613 100644\n--- a/sphinx/domains/std.py\n+++ b/sphinx/domains/std.py\n@@ -305,7 +305,7 @@ def make_glossary_term(env: \"BuildEnvironment\", textnodes: Iterable[Node], index\n         term['ids'].append(node_id)\n \n     std = cast(StandardDomain, env.get_domain('std'))\n-    std.note_object('term', termtext.lower(), node_id, location=term)\n+    std.note_object('term', termtext, node_id, location=term)\n \n     # add an index entry too\n     indexnode = addnodes.index()\n@@ -928,8 +928,20 @@ class StandardDomain(Domain):\n                           node: pending_xref, contnode: Element) -> Element:\n         objtypes = self.objtypes_for_role(typ) or []\n         for objtype in objtypes:\n-            if (objtype, target) in self.objects:\n-                docname, labelid = self.objects[objtype, target]\n+            if objtype == 'term':\n+                # For terms, lookup case-insensitively\n+                ltarget = target.lower()\n+                for (type, name), (docname, labelid) in self.objects.items():\n+                    if type == objtype and name.lower() == ltarget:\n+                        break\n+                else:\n+                    docname, labelid = '', ''\n+            else:\n+                if (objtype, target) in self.objects:\n+                    docname, labelid = self.objects[objtype, target]\n+                else:\n+                    docname, labelid = '', ''\n+            if docname:\n                 break\n         else:\n             docname, labelid = '', ''\n@@ -951,14 +963,21 @@ class StandardDomain(Domain):\n                 results.append(('std:' + role, res))\n         # all others\n         for objtype in self.object_types:\n-            key = (objtype, target)\n             if objtype == 'term':\n-                key = (objtype, ltarget)\n-            if key in self.objects:\n-                docname, labelid = self.objects[key]\n-                results.append(('std:' + self.role_for_objtype(objtype),\n-                                make_refnode(builder, fromdocname, docname,\n-                                             labelid, contnode)))\n+                # For terms, lookup case-insensitively\n+                for (type, name), (docname, labelid) in self.objects.items():\n+                    if type == objtype and name.lower() == ltarget:\n+                        results.append(('std:' + self.role_for_objtype(objtype),\n+                                        make_refnode(builder, fromdocname, docname,\n+                                                     labelid, contnode)))\n+                        break\n+            else:\n+                key = (objtype, target)\n+                if key in self.objects:\n+                    docname, labelid = self.objects[key]\n+                    results.append(('std:' + self.role_for_objtype(objtype),\n+                                    make_refnode(builder, fromdocname, docname,\n+                                                 labelid, contnode)))\n         return results\n \n     def get_objects(self) -> Iterator[Tuple[str, str, str, str, str, int]]:\ndiff --git a/tox.ini b/tox.ini\nindex d9f040544..bf39854b6 100644\n--- a/tox.ini\n+++ b/tox.ini\n@@ -28,7 +28,7 @@ extras =\n setenv =\n     PYTHONWARNINGS = all,ignore::ImportWarning:importlib._bootstrap_external,ignore::DeprecationWarning:site,ignore::DeprecationWarning:distutils\n commands=\n-    pytest --durations 25 {posargs}\n+    pytest -rA --durations 25 {posargs}\n \n [testenv:flake8]\n basepython = python3\n"}
{"instance_id": "sympy__sympy-17318", "model_name_or_path": "devchain", "model_patch": "diff --git a/sympy/simplify/radsimp.py b/sympy/simplify/radsimp.py\nindex 216ae26204..de59422669 100644\n--- a/sympy/simplify/radsimp.py\n+++ b/sympy/simplify/radsimp.py\n@@ -1113,6 +1113,8 @@ def _split_gcd(*a):\n     >>> _split_gcd(55, 35, 22, 14, 77, 10)\n     (5, [55, 35, 10], [22, 14, 77])\n     \"\"\"\n+    if not a:\n+        return 1, [], []\n     g = a[0]\n     b1 = [g]\n     b2 = []\n"}
{"instance_id": "django__django-14404", "model_name_or_path": "devchain", "model_patch": "diff --git a/django/contrib/admin/sites.py b/django/contrib/admin/sites.py\nindex 59a4a0364d..74ef4c491e 100644\n--- a/django/contrib/admin/sites.py\n+++ b/django/contrib/admin/sites.py\n@@ -420,7 +420,7 @@ class AdminSite:\n     def catch_all_view(self, request, url):\n         if settings.APPEND_SLASH and not url.endswith('/'):\n             urlconf = getattr(request, 'urlconf', None)\n-            path = '%s/' % request.path_info\n+            path = '%s/' % request.path\n             try:\n                 match = resolve(path, urlconf)\n             except Resolver404:\n"}
{"instance_id": "sympy__sympy-18763", "model_name_or_path": "devchain", "model_patch": "diff --git a/sympy/printing/latex.py b/sympy/printing/latex.py\nindex 8743723ea0..969072f47f 100644\n--- a/sympy/printing/latex.py\n+++ b/sympy/printing/latex.py\n@@ -698,7 +698,7 @@ def _print_Derivative(self, expr):\n \n     def _print_Subs(self, subs):\n         expr, old, new = subs.args\n-        latex_expr = self._print(expr)\n+        latex_expr = self.parenthesize(expr, PRECEDENCE[\"Mul\"], strict=True)\n         latex_old = (self._print(e) for e in old)\n         latex_new = (self._print(e) for e in new)\n         latex_subs = r'\\\\ '.join(\n"}
{"instance_id": "django__django-14034", "model_name_or_path": "devchain", "model_patch": "diff --git a/django/forms/fields.py b/django/forms/fields.py\nindex 65d6a9ec82..193cfe7df2 100644\n--- a/django/forms/fields.py\n+++ b/django/forms/fields.py\n@@ -1025,6 +1025,11 @@ class MultiValueField(Field):\n             if not value or not [v for v in value if v not in self.empty_values]:\n                 if self.required:\n                     raise ValidationError(self.error_messages['required'], code='required')\n+                # If not all fields are required, check if any individual field is required.\n+                # If so, don't return early - let the per-field validation handle it.\n+                elif not self.require_all_fields and any(f.required for f in self.fields):\n+                    # Continue to validate individual fields\n+                    pass\n                 else:\n                     return self.compress([])\n         else:\n"}
{"instance_id": "django__django-16263", "model_name_or_path": "devchain", "model_patch": "diff --git a/django/db/models/sql/query.py b/django/db/models/sql/query.py\nindex 9735ce10c8..dd3a32e611 100644\n--- a/django/db/models/sql/query.py\n+++ b/django/db/models/sql/query.py\n@@ -545,11 +545,79 @@ class Query(BaseExpression):\n \n         return dict(zip(outer_query.annotation_select, result))\n \n+    def _get_annotation_refs(self, expr, check_identity=True):\n+        \"\"\"\n+        Return a set of annotation names referenced by the given expression.\n+        If check_identity is True, also check if expr is the same object as\n+        an annotation (by object identity).\n+        \"\"\"\n+        refs = set()\n+        if isinstance(expr, Ref):\n+            refs.add(expr.refs)\n+        # Check if this expression is exactly the same object as an annotation\n+        if check_identity:\n+            for name, annotation in self.annotations.items():\n+                if expr is annotation:\n+                    refs.add(name)\n+        if hasattr(expr, \"get_source_expressions\"):\n+            for source in expr.get_source_expressions():\n+                refs.update(self._get_annotation_refs(source, check_identity))\n+        return refs\n+\n+    def _get_used_annotation_names(self):\n+        \"\"\"\n+        Return the set of annotation names that are used by the query.\n+        An annotation is used if it's referenced by:\n+        - A filter (where clause)\n+        - An ordering\n+        - Another annotation (transitively)\n+        \"\"\"\n+        used_names = set()\n+\n+        # Check where clause for annotation references\n+        if self.where:\n+            for child in self.where.children:\n+                used_names.update(self._get_annotation_refs(child))\n+\n+        # Check order_by for annotation references\n+        for item in self.order_by:\n+            if isinstance(item, str):\n+                if item.startswith(\"-\"):\n+                    item = item[1:]\n+                if item in self.annotations:\n+                    used_names.add(item)\n+            elif hasattr(item, \"get_source_expressions\"):\n+                for source in item.get_source_expressions():\n+                    used_names.update(self._get_annotation_refs(source))\n+\n+        # Transitively check if referenced annotations reference others\n+        to_process = list(used_names)\n+        while to_process:\n+            name = to_process.pop()\n+            if name in self.annotations:\n+                for ref in self._get_annotation_refs(self.annotations[name]):\n+                    if ref not in used_names:\n+                        used_names.add(ref)\n+                        to_process.append(ref)\n+\n+        return used_names\n+\n     def get_count(self, using):\n         \"\"\"\n         Perform a COUNT() query using the current filter constraints.\n         \"\"\"\n         obj = self.clone()\n+        # Find which annotations are used by filters, ordering, or other annotations.\n+        used_names = obj._get_used_annotation_names()\n+        unused_names = set(obj.annotations.keys()) - used_names\n+\n+        # Set the annotation_select_mask to exclude unused annotations.\n+        # This keeps the annotations in the dict (so get_aggregation uses the\n+        # subquery path when needed) but excludes them from the SELECT clause.\n+        if unused_names:\n+            # The mask should include only used annotations (and later __count)\n+            obj.set_annotation_mask(used_names)\n+\n         obj.add_annotation(Count(\"*\"), alias=\"__count\", is_summary=True)\n         return obj.get_aggregation(using, [\"__count\"])[\"__count\"]\n \n"}
{"instance_id": "matplotlib__matplotlib-23299", "model_name_or_path": "devchain", "model_patch": "diff --git a/lib/matplotlib/__init__.py b/lib/matplotlib/__init__.py\nindex c268a56724..d4c696b2ab 100644\n--- a/lib/matplotlib/__init__.py\n+++ b/lib/matplotlib/__init__.py\n@@ -674,7 +674,11 @@ class RcParams(MutableMapping, dict):\n             val = dict.__getitem__(self, key)\n             if val is rcsetup._auto_backend_sentinel:\n                 from matplotlib import pyplot as plt\n-                plt.switch_backend(rcsetup._auto_backend_sentinel)\n+                # Only trigger backend resolution if the backend hasn't been\n+                # loaded yet. If the backend is already loaded, calling\n+                # switch_backend() would close all existing figures.\n+                if plt._backend_mod is None:\n+                    plt.switch_backend(rcsetup._auto_backend_sentinel)\n \n         return dict.__getitem__(self, key)\n \ndiff --git a/lib/matplotlib/pyplot.py b/lib/matplotlib/pyplot.py\nindex e3c66ea67c..2f46a3a3ee 100644\n--- a/lib/matplotlib/pyplot.py\n+++ b/lib/matplotlib/pyplot.py\n@@ -222,7 +222,10 @@ def switch_backend(newbackend):\n     global _backend_mod\n     # make sure the init is pulled up so we can assign to it later\n     import matplotlib.backends\n-    close(\"all\")\n+    # Only close figures when actually switching backends, not during initial\n+    # backend load.\n+    if _backend_mod is not None:\n+        close(\"all\")\n \n     if newbackend is rcsetup._auto_backend_sentinel:\n         current_framework = cbook._get_running_interactive_framework()\n"}
{"instance_id": "psf__requests-6028", "model_name_or_path": "devchain", "model_patch": "diff --git a/requests/utils.py b/requests/utils.py\nindex 1c2ae4e0..153776c7 100644\n--- a/requests/utils.py\n+++ b/requests/utils.py\n@@ -974,6 +974,10 @@ def prepend_scheme_if_needed(url, new_scheme):\n     if not netloc:\n         netloc, path = path, netloc\n \n+    if auth:\n+        # parse_url doesn't provide the netloc with auth\n+        # so we'll add it ourselves.\n+        netloc = '@'.join([auth, netloc])\n     if scheme is None:\n         scheme = new_scheme\n     if path is None:\n"}
{"instance_id": "sympy__sympy-15976", "model_name_or_path": "devchain", "model_patch": "diff --git a/sympy/printing/mathml.py b/sympy/printing/mathml.py\nindex 3f8b8d49e0..6f964b3945 100644\n--- a/sympy/printing/mathml.py\n+++ b/sympy/printing/mathml.py\n@@ -743,11 +743,6 @@ def _print_Sum(self, e):\n         return mrow\n \n     def _print_Symbol(self, sym, style='plain'):\n-        x = self.dom.createElement('mi')\n-\n-        if style == 'bold':\n-            x.setAttribute('mathvariant', 'bold')\n-\n         def join(items):\n             if len(items) > 1:\n                 mrow = self.dom.createElement('mrow')\n@@ -778,28 +773,30 @@ def translate(s):\n         subs = [translate(sub) for sub in subs]\n \n         mname = self.dom.createElement('mi')\n+        if style == 'bold':\n+            mname.setAttribute('mathvariant', 'bold')\n         mname.appendChild(self.dom.createTextNode(name))\n+\n         if len(supers) == 0:\n             if len(subs) == 0:\n-                x.appendChild(self.dom.createTextNode(name))\n+                return mname\n             else:\n                 msub = self.dom.createElement('msub')\n                 msub.appendChild(mname)\n                 msub.appendChild(join(subs))\n-                x.appendChild(msub)\n+                return msub\n         else:\n             if len(subs) == 0:\n                 msup = self.dom.createElement('msup')\n                 msup.appendChild(mname)\n                 msup.appendChild(join(supers))\n-                x.appendChild(msup)\n+                return msup\n             else:\n                 msubsup = self.dom.createElement('msubsup')\n                 msubsup.appendChild(mname)\n                 msubsup.appendChild(join(subs))\n                 msubsup.appendChild(join(supers))\n-                x.appendChild(msubsup)\n-        return x\n+                return msubsup\n \n     def _print_MatrixSymbol(self, sym):\n         return self._print_Symbol(sym, style=self._settings['mat_symbol_style'])\n"}
{"instance_id": "django__django-13344", "model_name_or_path": "devchain", "model_patch": "diff --git a/django/core/handlers/base.py b/django/core/handlers/base.py\nindex 169df6b3cf..301564cd1c 100644\n--- a/django/core/handlers/base.py\n+++ b/django/core/handlers/base.py\n@@ -45,10 +45,10 @@ class BaseHandler:\n                     'Middleware %s must have at least one of '\n                     'sync_capable/async_capable set to True.' % middleware_path\n                 )\n-            elif not handler_is_async and middleware_can_sync:\n-                middleware_is_async = False\n+            elif middleware_can_async:\n+                middleware_is_async = True\n             else:\n-                middleware_is_async = middleware_can_async\n+                middleware_is_async = False\n             try:\n                 # Adapt handler, if needed.\n                 handler = self.adapt_method_mode(\n"}
{"instance_id": "matplotlib__matplotlib-25479", "model_name_or_path": "devchain", "model_patch": "diff --git a/lib/matplotlib/cm.py b/lib/matplotlib/cm.py\nindex 38158db956..35b7a5f95f 100644\n--- a/lib/matplotlib/cm.py\n+++ b/lib/matplotlib/cm.py\n@@ -145,7 +145,9 @@ class ColormapRegistry(Mapping):\n             _api.warn_external(f\"Overwriting the cmap {name!r} \"\n                                \"that was already in the registry.\")\n \n-        self._cmaps[name] = cmap.copy()\n+        cmap_copy = cmap.copy()\n+        cmap_copy.name = name\n+        self._cmaps[name] = cmap_copy\n \n     def unregister(self, name):\n         \"\"\"\n"}
{"instance_id": "django__django-14155", "model_name_or_path": "devchain", "model_patch": "diff --git a/django/urls/resolvers.py b/django/urls/resolvers.py\nindex 9b00e24509..959cca1a90 100644\n--- a/django/urls/resolvers.py\n+++ b/django/urls/resolvers.py\n@@ -45,6 +45,9 @@ class ResolverMatch:\n         self.namespaces = [x for x in namespaces if x] if namespaces else []\n         self.namespace = ':'.join(self.namespaces)\n \n+        # Unwrap functools.partial to get the underlying function for the path.\n+        if isinstance(func, functools.partial):\n+            func = func.func\n         if not hasattr(func, '__name__'):\n             # A class-based view\n             self._func_path = func.__class__.__module__ + '.' + func.__class__.__name__\n"}
{"instance_id": "django__django-11820", "model_name_or_path": "devchain", "model_patch": "diff --git a/django/db/models/base.py b/django/db/models/base.py\nindex 3ce711f3f1..9ea8e443fe 100644\n--- a/django/db/models/base.py\n+++ b/django/db/models/base.py\n@@ -1707,6 +1707,9 @@ class Model(metaclass=ModelBase):\n             _cls = cls\n             fld = None\n             for part in field.split(LOOKUP_SEP):\n+                # Handle 'pk' - it's an alias for the primary key field name.\n+                if part == 'pk':\n+                    part = _cls._meta.pk.name\n                 try:\n                     fld = _cls._meta.get_field(part)\n                     if fld.is_relation:\n"}
{"instance_id": "django__django-13809", "model_name_or_path": "devchain", "model_patch": "diff --git a/django/core/management/commands/runserver.py b/django/core/management/commands/runserver.py\nindex d9fb088350..d5a037a824 100644\n--- a/django/core/management/commands/runserver.py\n+++ b/django/core/management/commands/runserver.py\n@@ -51,6 +51,10 @@ class Command(BaseCommand):\n             '--noreload', action='store_false', dest='use_reloader',\n             help='Tells Django to NOT use the auto-reloader.',\n         )\n+        parser.add_argument(\n+            '--skip-checks', action='store_true',\n+            help='Skip system checks.',\n+        )\n \n     def execute(self, *args, **options):\n         if options['no_color']:\n@@ -114,11 +118,13 @@ class Command(BaseCommand):\n         shutdown_message = options.get('shutdown_message', '')\n         quit_command = 'CTRL-BREAK' if sys.platform == 'win32' else 'CONTROL-C'\n \n-        self.stdout.write(\"Performing system checks...\\n\\n\")\n-        self.check(display_num_errors=True)\n-        # Need to check migrations here, so can't use the\n-        # requires_migrations_check attribute.\n-        self.check_migrations()\n+        skip_checks = options.get('skip_checks')\n+        if not skip_checks:\n+            self.stdout.write(\"Performing system checks...\\n\\n\")\n+            self.check(display_num_errors=True)\n+            # Need to check migrations here, so can't use the\n+            # requires_migrations_check attribute.\n+            self.check_migrations()\n         now = datetime.now().strftime('%B %d, %Y - %X')\n         self.stdout.write(now)\n         self.stdout.write((\n"}
{"instance_id": "sphinx-doc__sphinx-11510", "model_name_or_path": "devchain", "model_patch": "diff --git a/sphinx/directives/other.py b/sphinx/directives/other.py\nindex 19f7c0a8d..9885aed6a 100644\n--- a/sphinx/directives/other.py\n+++ b/sphinx/directives/other.py\n@@ -1,9 +1,10 @@\n from __future__ import annotations\n \n import re\n+from pathlib import Path\n from typing import TYPE_CHECKING, Any, cast\n \n-from docutils import nodes\n+from docutils import io, nodes, statemachine, utils\n from docutils.parsers.rst import directives\n from docutils.parsers.rst.directives.admonitions import BaseAdmonition\n from docutils.parsers.rst.directives.misc import Class\n@@ -376,8 +377,103 @@ class Include(BaseInclude, SphinxDirective):\n         rel_filename, filename = self.env.relfn2path(self.arguments[0])\n         self.arguments[0] = filename\n         self.env.note_included(filename)\n+\n+        # For standard RST includes (not literal, code, or parser options),\n+        # we need to emit the source-read event for the included file\n+        if ('literal' not in self.options and\n+                'code' not in self.options and\n+                'parser' not in self.options):\n+            return self._run_with_source_read(filename, rel_filename)\n+\n         return super().run()\n \n+    def _run_with_source_read(self, filename: str, rel_filename: str) -> list[Node]:\n+        \"\"\"Process the include directive with source-read event emission.\"\"\"\n+        from docutils.parsers.rst import states\n+\n+        # Get the path for the included file\n+        path = Path(filename)\n+        current_source = self.state.document.current_source\n+        encoding = self.options.get(\n+            'encoding', self.state.document.settings.input_encoding)\n+        e_handler = self.state.document.settings.input_encoding_error_handler\n+        tab_width = self.options.get(\n+            'tab-width', self.state.document.settings.tab_width)\n+\n+        # Open and read the file\n+        try:\n+            include_file = io.FileInput(source_path=path,\n+                                        encoding=encoding,\n+                                        error_handler=e_handler)\n+        except OSError as error:\n+            raise self.severe('Problems with \"%s\" directive path:\\n%s.' %\n+                              (self.name, io.error_string(error)))\n+        else:\n+            self.state.document.settings.record_dependencies.add(path)\n+\n+        # Read the file content\n+        startline = self.options.get('start-line', None)\n+        endline = self.options.get('end-line', None)\n+        try:\n+            if startline or (endline is not None):\n+                lines = include_file.readlines()\n+                rawtext = ''.join(lines[startline:endline])\n+            else:\n+                rawtext = include_file.read()\n+        except UnicodeError as error:\n+            raise self.severe('Problem with \"%s\" directive:\\n%s'\n+                              % (self.name, io.error_string(error)))\n+\n+        # Apply start-after/end-before options\n+        after_text = self.options.get('start-after', None)\n+        if after_text:\n+            after_index = rawtext.find(after_text)\n+            if after_index < 0:\n+                raise self.severe('Problem with \"start-after\" option of \"%s\" '\n+                                  'directive:\\nText not found.' % self.name)\n+            rawtext = rawtext[after_index + len(after_text):]\n+\n+        before_text = self.options.get('end-before', None)\n+        if before_text:\n+            before_index = rawtext.find(before_text)\n+            if before_index < 0:\n+                raise self.severe('Problem with \"end-before\" option of \"%s\" '\n+                                  'directive:\\nText not found.' % self.name)\n+            rawtext = rawtext[:before_index]\n+\n+        # Emit the source-read event for the included file\n+        docname = self.env.path2doc(filename)\n+        if docname:\n+            arg = [rawtext]\n+            self.env.events.emit('source-read', docname, arg)\n+            rawtext = arg[0]\n+\n+        # Convert to lines and check line length limit\n+        include_lines = statemachine.string2lines(rawtext, tab_width,\n+                                                  convert_whitespace=True)\n+        for i, line in enumerate(include_lines):\n+            if len(line) > self.state.document.settings.line_length_limit:\n+                raise self.warning('\"%s\": line %d exceeds the'\n+                                   ' line-length-limit.' % (filename, i + 1))\n+\n+        # Prevent circular inclusion\n+        clip_options = (startline, endline, before_text, after_text)\n+        include_log = self.state.document.include_log\n+        if not include_log:\n+            include_log.append((utils.relative_path(None, current_source),\n+                                (None, None, None, None)))\n+        if (filename, clip_options) in include_log:\n+            master_paths = (pth for (pth, opt) in reversed(include_log))\n+            inclusion_chain = '\\n> '.join((filename, *master_paths))\n+            raise self.warning('circular inclusion in \"%s\" directive:\\n%s'\n+                               % (self.name, inclusion_chain))\n+\n+        # Include as RST source\n+        include_lines += ['', '.. end of inclusion from \"%s\"' % filename]\n+        self.state_machine.insert_input(include_lines, filename)\n+        include_log.append((filename, clip_options))\n+        return []\n+\n \n def setup(app: Sphinx) -> dict[str, Any]:\n     directives.register_directive('toctree', TocTree)\ndiff --git a/tox.ini b/tox.ini\nindex a3dc56211..c76147de3 100644\n--- a/tox.ini\n+++ b/tox.ini\n@@ -24,7 +24,7 @@ setenv =\n     PYTHONWARNINGS = error\n     PYTEST_ADDOPTS = {env:PYTEST_ADDOPTS:} --color yes\n commands=\n-    python -X dev -X warn_default_encoding -m pytest --durations 25 {posargs}\n+    python -X dev -X warn_default_encoding -m pytest -rA --durations 25 {posargs}\n \n [testenv:docs]\n basepython = python3\n"}
{"instance_id": "django__django-16667", "model_name_or_path": "devchain", "model_patch": "diff --git a/django/forms/widgets.py b/django/forms/widgets.py\nindex 842b73e95c..f4bc74b7cd 100644\n--- a/django/forms/widgets.py\n+++ b/django/forms/widgets.py\n@@ -1157,7 +1157,7 @@ class SelectDateWidget(Widget):\n             input_format = formats.sanitize_strftime_format(input_format)\n             try:\n                 date_value = datetime.date(int(y), int(m), int(d))\n-            except ValueError:\n+            except (ValueError, OverflowError):\n                 # Return pseudo-ISO dates with zeros for any unselected values,\n                 # e.g. '2017-0-23'.\n                 return \"%s-%s-%s\" % (y or 0, m or 0, d or 0)\n"}
{"instance_id": "matplotlib__matplotlib-24970", "model_name_or_path": "devchain", "model_patch": "diff --git a/lib/matplotlib/colors.py b/lib/matplotlib/colors.py\nindex 2d3072da04..a9cc5b88eb 100644\n--- a/lib/matplotlib/colors.py\n+++ b/lib/matplotlib/colors.py\n@@ -725,6 +725,10 @@ class Colormap:\n                 # Avoid converting large positive values to negative integers.\n                 np.clip(xa, -1, self.N, out=xa)\n                 xa = xa.astype(int)\n+        # Ensure integer arrays can hold the special index values.\n+        if xa.dtype.kind in (\"i\", \"u\"):\n+            if np.iinfo(xa.dtype).max < self.N + 2:\n+                xa = xa.astype(int)\n         # Set the over-range indices before the under-range;\n         # otherwise the under-range values get converted to over-range.\n         xa[xa > self.N - 1] = self._i_over\n"}
{"instance_id": "pylint-dev__pylint-4661", "model_name_or_path": "devchain", "model_patch": "diff --git a/pylint/config/__init__.py b/pylint/config/__init__.py\nindex 84774d0e3..aa57dd275 100644\n--- a/pylint/config/__init__.py\n+++ b/pylint/config/__init__.py\n@@ -63,7 +63,13 @@ if \"PYLINTHOME\" in os.environ:\n elif USER_HOME == \"~\":\n     PYLINT_HOME = \".pylint.d\"\n else:\n-    PYLINT_HOME = os.path.join(USER_HOME, \".pylint.d\")\n+    # XDG Base Directory Specification compliance\n+    # Use XDG_DATA_HOME if set, otherwise use default ~/.local/share\n+    xdg_data_home = os.environ.get(\"XDG_DATA_HOME\")\n+    if xdg_data_home:\n+        PYLINT_HOME = os.path.join(xdg_data_home, \"pylint\")\n+    else:\n+        PYLINT_HOME = os.path.join(USER_HOME, \".local\", \"share\", \"pylint\")\n \n \n def _get_pdata_path(base_name, recurs):\n@@ -83,7 +89,7 @@ def load_results(base):\n def save_results(results, base):\n     if not os.path.exists(PYLINT_HOME):\n         try:\n-            os.mkdir(PYLINT_HOME)\n+            os.makedirs(PYLINT_HOME)\n         except OSError:\n             print(\"Unable to create directory %s\" % PYLINT_HOME, file=sys.stderr)\n     data_file = _get_pdata_path(base, 1)\n@@ -110,8 +116,8 @@ ENV_HELP = (\n The following environment variables are used:\n     * PYLINTHOME\n     Path to the directory where persistent data for the run will be stored. If\n-not found, it defaults to ~/.pylint.d/ or .pylint.d (in the current working\n-directory).\n+not found, it defaults to $XDG_DATA_HOME/pylint or ~/.local/share/pylint\n+(respecting the XDG Base Directory Specification).\n     * PYLINTRC\n     Path to the configuration file. See the documentation for the method used\n to search for configuration file.\n"}
{"instance_id": "astropy__astropy-13977", "model_name_or_path": "devchain", "model_patch": "diff --git a/astropy/units/quantity.py b/astropy/units/quantity.py\nindex b98abfafb0..67fb18ab3a 100644\n--- a/astropy/units/quantity.py\n+++ b/astropy/units/quantity.py\n@@ -667,7 +667,15 @@ class Quantity(np.ndarray):\n         arrays = []\n         for input_, converter in zip(inputs, converters):\n             input_ = getattr(input_, \"value\", input_)\n-            arrays.append(converter(input_) if converter else input_)\n+            if converter:\n+                try:\n+                    arrays.append(converter(input_))\n+                except (TypeError, ValueError):\n+                    # Incompatible type; return NotImplemented so the other\n+                    # operand can try its reflected operation (e.g., __radd__).\n+                    return NotImplemented\n+            else:\n+                arrays.append(input_)\n \n         # Call our superclass's __array_ufunc__\n         result = super().__array_ufunc__(function, method, *arrays, **kwargs)\n"}
{"instance_id": "pydata__xarray-6599", "model_name_or_path": "devchain", "model_patch": "diff --git a/xarray/core/computation.py b/xarray/core/computation.py\nindex 823cbe02..d93fe854 100644\n--- a/xarray/core/computation.py\n+++ b/xarray/core/computation.py\n@@ -1905,6 +1905,10 @@ def polyval(\n     coeffs = coeffs.reindex(\n         {degree_dim: np.arange(max_deg + 1)}, fill_value=0, copy=False\n     )\n+    # If coord is a DataArray with a named dimension, use its coordinate values\n+    # (like the old get_clean_interp_index behavior)\n+    if hasattr(coord, \"name\") and coord.name is not None and coord.name in getattr(coord, \"dims\", []):\n+        coord = coord.coords[coord.name]\n     coord = _ensure_numeric(coord)  # type: ignore # https://github.com/python/mypy/issues/1533 ?\n \n     # using Horner's method\n@@ -1933,7 +1937,8 @@ def _ensure_numeric(data: T_Xarray) -> T_Xarray:\n     from .dataset import Dataset\n \n     def to_floatable(x: DataArray) -> DataArray:\n-        if x.dtype.kind in \"mM\":\n+        if x.dtype.kind == \"M\":\n+            # datetime64: use epoch as offset\n             return x.copy(\n                 data=datetime_to_numeric(\n                     x.data,\n@@ -1941,6 +1946,15 @@ def _ensure_numeric(data: T_Xarray) -> T_Xarray:\n                     datetime_unit=\"ns\",\n                 ),\n             )\n+        elif x.dtype.kind == \"m\":\n+            # timedelta64: use None as offset (will use minimum value)\n+            return x.copy(\n+                data=datetime_to_numeric(\n+                    x.data,\n+                    offset=None,\n+                    datetime_unit=\"ns\",\n+                ),\n+            )\n         return x\n \n     if isinstance(data, Dataset):\n"}
{"instance_id": "sphinx-doc__sphinx-7462", "model_name_or_path": "devchain", "model_patch": "diff --git a/sphinx/domains/python.py b/sphinx/domains/python.py\nindex 3ca730e30..b6a228a60 100644\n--- a/sphinx/domains/python.py\n+++ b/sphinx/domains/python.py\n@@ -118,7 +118,8 @@ def _parse_annotation(annotation: str) -> List[Node]:\n             for elem in node.elts:\n                 result.extend(unparse(elem))\n                 result.append(addnodes.desc_sig_punctuation('', ', '))\n-            result.pop()\n+            if result:\n+                result.pop()\n             return result\n         else:\n             raise SyntaxError  # unsupported syntax\ndiff --git a/tox.ini b/tox.ini\nindex d9f040544..bf39854b6 100644\n--- a/tox.ini\n+++ b/tox.ini\n@@ -28,7 +28,7 @@ extras =\n setenv =\n     PYTHONWARNINGS = all,ignore::ImportWarning:importlib._bootstrap_external,ignore::DeprecationWarning:site,ignore::DeprecationWarning:distutils\n commands=\n-    pytest --durations 25 {posargs}\n+    pytest -rA --durations 25 {posargs}\n \n [testenv:flake8]\n basepython = python3\n"}
{"instance_id": "django__django-14315", "model_name_or_path": "devchain", "model_patch": "diff --git a/django/db/backends/base/client.py b/django/db/backends/base/client.py\nindex 339f1e863c..7ceff9e981 100644\n--- a/django/db/backends/base/client.py\n+++ b/django/db/backends/base/client.py\n@@ -23,4 +23,4 @@ class BaseDatabaseClient:\n         args, env = self.settings_to_cmd_args_env(self.connection.settings_dict, parameters)\n         if env:\n             env = {**os.environ, **env}\n-        subprocess.run(args, env=env, check=True)\n+        subprocess.run(args, env=env or None, check=True)\n"}
{"instance_id": "mwaskom__seaborn-3069", "model_name_or_path": "devchain", "model_patch": "diff --git a/seaborn/_core/plot.py b/seaborn/_core/plot.py\nindex 4f0290a4..5fec7458 100644\n--- a/seaborn/_core/plot.py\n+++ b/seaborn/_core/plot.py\n@@ -1475,21 +1475,23 @@ class Plotter:\n \n                 axes_df = self._filter_subplot_data(df, view)\n \n-                with pd.option_context(\"mode.use_inf_as_null\", True):\n-                    if keep_na:\n-                        # The simpler thing to do would be x.dropna().reindex(x.index).\n-                        # But that doesn't work with the way that the subset iteration\n-                        # is written below, which assumes data for grouping vars.\n-                        # Matplotlib (usually?) masks nan data, so this should \"work\".\n-                        # Downstream code can also drop these rows, at some speed cost.\n-                        present = axes_df.notna().all(axis=1)\n-                        nulled = {}\n-                        for axis in \"xy\":\n-                            if axis in axes_df:\n-                                nulled[axis] = axes_df[axis].where(present)\n-                        axes_df = axes_df.assign(**nulled)\n-                    else:\n-                        axes_df = axes_df.dropna()\n+                # Note: mode.use_inf_as_null was removed in pandas 1.3+\n+                # We'll handle inf values differently if needed, but for now\n+                # just skip the option_context to maintain compatibility\n+                if keep_na:\n+                    # The simpler thing to do would be x.dropna().reindex(x.index).\n+                    # But that doesn't work with the way that the subset iteration\n+                    # is written below, which assumes data for grouping vars.\n+                    # Matplotlib (usually?) masks nan data, so this should \"work\".\n+                    # Downstream code can also drop these rows, at some speed cost.\n+                    present = axes_df.notna().all(axis=1)\n+                    nulled = {}\n+                    for axis in \"xy\":\n+                        if axis in axes_df:\n+                            nulled[axis] = axes_df[axis].where(present)\n+                    axes_df = axes_df.assign(**nulled)\n+                else:\n+                    axes_df = axes_df.dropna()\n \n                 subplot_keys = {}\n                 for dim in [\"col\", \"row\"]:\n@@ -1627,6 +1629,8 @@ class Plotter:\n \n     def _finalize_figure(self, p: Plot) -> None:\n \n+        from seaborn._core.scales import Nominal\n+\n         for sub in self._subplots:\n             ax = sub[\"ax\"]\n             for axis in \"xy\":\n@@ -1644,6 +1648,23 @@ class Plotter:\n                         hi = cast(float, hi) + 0.5\n                     ax.set(**{f\"{axis}lim\": (lo, hi)})\n \n+                # Handle Nominal (categorical) scales\n+                # Apply same behavior as traditional categorical plots:\n+                # 1. Disable grid on categorical axis\n+                # 2. Set limits to +/- 0.5 from first/last tick (only if not explicitly set)\n+                # 3. Invert y axis for categorical variables\n+                scale = self._scales.get(axis_key)\n+                if isinstance(scale, Nominal):\n+                    getattr(ax, f\"{axis}axis\").grid(False)\n+                    # Only set limits if user hasn't explicitly set them\n+                    if axis_key not in p._limits:\n+                        n = len(getattr(ax, f\"get_{axis}ticks\")())\n+                        if axis == \"x\":\n+                            ax.set_xlim(-.5, n - .5, auto=None)\n+                        else:\n+                            # Note limits that correspond to inverted y axis\n+                            ax.set_ylim(n - .5, -.5, auto=None)\n+\n         engine_default = None if p._target is not None else \"tight\"\n         layout_engine = p._layout_spec.get(\"engine\", engine_default)\n         set_layout_engine(self._figure, layout_engine)\n"}
{"instance_id": "pytest-dev__pytest-10356", "model_name_or_path": "devchain", "model_patch": "diff --git a/src/_pytest/mark/structures.py b/src/_pytest/mark/structures.py\nindex 800a25c92..8393be27a 100644\n--- a/src/_pytest/mark/structures.py\n+++ b/src/_pytest/mark/structures.py\n@@ -356,10 +356,31 @@ class MarkDecorator:\n \n \n def get_unpacked_marks(obj: object) -> Iterable[Mark]:\n-    \"\"\"Obtain the unpacked marks that are stored on an object.\"\"\"\n-    mark_list = getattr(obj, \"pytestmark\", [])\n-    if not isinstance(mark_list, list):\n-        mark_list = [mark_list]\n+    \"\"\"Obtain the unpacked marks that are stored on an object.\n+\n+    For classes, this also considers marks from base classes according to\n+    the MRO (Method Resolution Order), enabling marks from all base classes\n+    to be inherited (e.g., when using multiple inheritance).\n+    \"\"\"\n+    if isinstance(obj, type):\n+        # For classes, collect marks from the class's own __dict__ and all\n+        # base classes' __dict__ (not via getattr, to avoid MRO issues)\n+        mark_list = []\n+        # Iterate in reverse MRO order so derived class marks come last\n+        for basecls in reversed(obj.__mro__):\n+            # Look directly in __dict__ to avoid MRO attribute lookup which\n+            # would stop at the first parent with pytestmark\n+            base_marks = basecls.__dict__.get(\"pytestmark\")\n+            if base_marks is not None:\n+                if not isinstance(base_marks, list):\n+                    base_marks = [base_marks]\n+                mark_list.extend(base_marks)\n+    else:\n+        # For functions and other objects, use normal attribute lookup\n+        mark_list = getattr(obj, \"pytestmark\", [])\n+        if not isinstance(mark_list, list):\n+            mark_list = [mark_list]\n+\n     return normalize_mark_list(mark_list)\n \n \n@@ -388,7 +409,21 @@ def store_mark(obj, mark: Mark) -> None:\n     assert isinstance(mark, Mark), mark\n     # Always reassign name to avoid updating pytestmark in a reference that\n     # was only borrowed.\n-    obj.pytestmark = [*get_unpacked_marks(obj), mark]\n+    # For classes, only look at the class's own marks (not inherited),\n+    # to avoid duplicating marks from the MRO.\n+    if isinstance(obj, type):\n+        existing_mark_list = obj.__dict__.get(\"pytestmark\")\n+    else:\n+        existing_mark_list = getattr(obj, \"pytestmark\", None)\n+\n+    if existing_mark_list is None:\n+        existing_marks = []\n+    elif not isinstance(existing_mark_list, list):\n+        existing_marks = [existing_mark_list]\n+    else:\n+        existing_marks = list(existing_mark_list)\n+\n+    obj.pytestmark = [*existing_marks, mark]\n \n \n # Typing for builtin pytest marks. This is cheating; it gives builtin marks\n"}
{"instance_id": "django__django-13513", "model_name_or_path": "devchain", "model_patch": "diff --git a/django/views/debug.py b/django/views/debug.py\nindex 7a89f7bd15..68dba4b500 100644\n--- a/django/views/debug.py\n+++ b/django/views/debug.py\n@@ -91,19 +91,18 @@ class SafeExceptionReporterFilter:\n         value is a dictionary, recursively cleanse the keys in that dictionary.\n         \"\"\"\n         try:\n-            is_sensitive = self.hidden_settings.search(key)\n+            if self.hidden_settings.search(key):\n+                cleansed = self.cleansed_substitute\n+            elif isinstance(value, dict):\n+                cleansed = {k: self.cleanse_setting(k, v) for k, v in value.items()}\n+            elif isinstance(value, list):\n+                cleansed = [self.cleanse_setting('', v) for v in value]\n+            elif isinstance(value, tuple):\n+                cleansed = tuple([self.cleanse_setting('', v) for v in value])\n+            else:\n+                cleansed = value\n         except TypeError:\n-            is_sensitive = False\n-\n-        if is_sensitive:\n-            cleansed = self.cleansed_substitute\n-        elif isinstance(value, dict):\n-            cleansed = {k: self.cleanse_setting(k, v) for k, v in value.items()}\n-        elif isinstance(value, list):\n-            cleansed = [self.cleanse_setting('', v) for v in value]\n-        elif isinstance(value, tuple):\n-            cleansed = tuple([self.cleanse_setting('', v) for v in value])\n-        else:\n+            # If the key isn't regex-able, just return as-is.\n             cleansed = value\n \n         if callable(cleansed):\n@@ -481,10 +480,8 @@ def technical_404_response(request, exception):\n     try:\n         tried = exception.args[0]['tried']\n     except (IndexError, TypeError, KeyError):\n-        resolved = True\n-        tried = request.resolver_match.tried if request.resolver_match else None\n+        tried = []\n     else:\n-        resolved = False\n         if (not tried or (                  # empty URLconf\n             request.path == '/' and\n             len(tried) == 1 and             # default URLconf\n@@ -522,7 +519,6 @@ def technical_404_response(request, exception):\n         'root_urlconf': settings.ROOT_URLCONF,\n         'request_path': error_url,\n         'urlpatterns': tried,\n-        'resolved': resolved,\n         'reason': str(exception),\n         'request': request,\n         'settings': reporter_filter.get_safe_settings(),\n"}
{"instance_id": "django__django-12273", "model_name_or_path": "devchain", "model_patch": "diff --git a/django/db/models/base.py b/django/db/models/base.py\nindex 844c01e95e..b07e7783a8 100644\n--- a/django/db/models/base.py\n+++ b/django/db/models/base.py\n@@ -569,7 +569,20 @@ class Model(metaclass=ModelBase):\n         return getattr(self, meta.pk.attname)\n \n     def _set_pk_val(self, value):\n-        return setattr(self, self._meta.pk.attname, value)\n+        setattr(self, self._meta.pk.attname, value)\n+        # When setting the primary key to None, also set parent primary keys to\n+        # None. This ensures that all tables in a multi-table inheritance\n+        # hierarchy will get new primary keys on save, not just the child table.\n+        if value is None:\n+            parents = self._meta.parents\n+            # Process all parent models recursively to handle multi-level inheritance\n+            while parents:\n+                new_parents = {}\n+                for parent, field in parents.items():\n+                    setattr(self, parent._meta.pk.attname, None)\n+                    # Collect this parent's parents for the next iteration\n+                    new_parents.update(parent._meta.parents)\n+                parents = new_parents\n \n     pk = property(_get_pk_val, _set_pk_val)\n \n@@ -803,6 +816,14 @@ class Model(metaclass=ModelBase):\n         meta = cls._meta\n         inserted = False\n         for parent, field in meta.parents.items():\n+            # When a parent PK is set to None (e.g., obj.pk = None to create a\n+            # new object), also set the child's FK to None so both parent and\n+            # child get new primary keys on save. Check if the PK attribute is\n+            # in __dict__ to distinguish between \"explicitly set to None\" and\n+            # \"never set\".\n+            pk_attname = parent._meta.pk.attname\n+            if field and pk_attname in self.__dict__ and getattr(self, pk_attname) is None:\n+                setattr(self, field.attname, None)\n             # Make sure the link fields are synced between parent and self.\n             if (field and getattr(self, parent._meta.pk.attname) is None and\n                     getattr(self, field.attname) is not None):\n"}
{"instance_id": "django__django-16502", "model_name_or_path": "devchain", "model_patch": "diff --git a/django/core/handlers/wsgi.py b/django/core/handlers/wsgi.py\nindex c2b7cc2b6f..591ec8a1b5 100644\n--- a/django/core/handlers/wsgi.py\n+++ b/django/core/handlers/wsgi.py\n@@ -133,6 +133,20 @@ class WSGIHandler(base.BaseHandler):\n         request = self.request_class(environ)\n         response = self.get_response(request)\n \n+        # Simulate the behavior of most web servers by removing the content\n+        # of responses for HEAD requests, 1xx, 204, and 304 responses.\n+        # Ensures compliance with RFC 9112 Section 6.3.\n+        if 100 <= response.status_code < 200 or response.status_code in (204, 304):\n+            if response.streaming:\n+                response.streaming_content = []\n+            else:\n+                response.content = b\"\"\n+        elif request.method == \"HEAD\":\n+            if response.streaming:\n+                response.streaming_content = []\n+            else:\n+                response.content = b\"\"\n+\n         response._handler_class = self.__class__\n \n         status = \"%d %s\" % (response.status_code, response.reason_phrase)\n"}
{"instance_id": "django__django-14725", "model_name_or_path": "devchain", "model_patch": "diff --git a/django/forms/formsets.py b/django/forms/formsets.py\nindex 75b0646512..92832ed886 100644\n--- a/django/forms/formsets.py\n+++ b/django/forms/formsets.py\n@@ -478,7 +478,7 @@ class BaseFormSet(RenderableFormMixin):\n def formset_factory(form, formset=BaseFormSet, extra=1, can_order=False,\n                     can_delete=False, max_num=None, validate_max=False,\n                     min_num=None, validate_min=False, absolute_max=None,\n-                    can_delete_extra=True, renderer=None):\n+                    can_delete_extra=True, can_add=True, renderer=None):\n     \"\"\"Return a FormSet for the given form class.\"\"\"\n     if min_num is None:\n         min_num = DEFAULT_MIN_NUM\n@@ -499,6 +499,7 @@ def formset_factory(form, formset=BaseFormSet, extra=1, can_order=False,\n         'can_order': can_order,\n         'can_delete': can_delete,\n         'can_delete_extra': can_delete_extra,\n+        'can_add': can_add,\n         'min_num': min_num,\n         'max_num': max_num,\n         'absolute_max': absolute_max,\ndiff --git a/django/forms/models.py b/django/forms/models.py\nindex 7effb202e3..6a277bd772 100644\n--- a/django/forms/models.py\n+++ b/django/forms/models.py\n@@ -813,6 +813,8 @@ class BaseModelFormSet(BaseFormSet):\n \n     def save_new_objects(self, commit=True):\n         self.new_objects = []\n+        if not self.can_add:\n+            return self.new_objects\n         for form in self.extra_forms:\n             if not form.has_changed():\n                 continue\n@@ -875,7 +877,8 @@ def modelformset_factory(model, form=ModelForm, formfield_callback=None,\n                          widgets=None, validate_max=False, localized_fields=None,\n                          labels=None, help_texts=None, error_messages=None,\n                          min_num=None, validate_min=False, field_classes=None,\n-                         absolute_max=None, can_delete_extra=True, renderer=None):\n+                         absolute_max=None, can_delete_extra=True, can_add=True,\n+                         renderer=None):\n     \"\"\"Return a FormSet class for the given Django model class.\"\"\"\n     meta = getattr(form, 'Meta', None)\n     if (getattr(meta, 'fields', fields) is None and\n@@ -894,7 +897,7 @@ def modelformset_factory(model, form=ModelForm, formfield_callback=None,\n                               can_order=can_order, can_delete=can_delete,\n                               validate_min=validate_min, validate_max=validate_max,\n                               absolute_max=absolute_max, can_delete_extra=can_delete_extra,\n-                              renderer=renderer)\n+                              can_add=can_add, renderer=renderer)\n     FormSet.model = model\n     return FormSet\n \n@@ -1076,7 +1079,8 @@ def inlineformset_factory(parent_model, model, form=ModelForm,\n                           widgets=None, validate_max=False, localized_fields=None,\n                           labels=None, help_texts=None, error_messages=None,\n                           min_num=None, validate_min=False, field_classes=None,\n-                          absolute_max=None, can_delete_extra=True, renderer=None):\n+                          absolute_max=None, can_delete_extra=True, can_add=True,\n+                          renderer=None):\n     \"\"\"\n     Return an ``InlineFormSet`` for the given kwargs.\n \n@@ -1108,6 +1112,7 @@ def inlineformset_factory(parent_model, model, form=ModelForm,\n         'field_classes': field_classes,\n         'absolute_max': absolute_max,\n         'can_delete_extra': can_delete_extra,\n+        'can_add': can_add,\n         'renderer': renderer,\n     }\n     FormSet = modelformset_factory(model, **kwargs)\n"}
{"instance_id": "pydata__xarray-6992", "model_name_or_path": "devchain", "model_patch": "diff --git a/xarray/core/dataset.py b/xarray/core/dataset.py\nindex 620f32f8..441c1698 100644\n--- a/xarray/core/dataset.py\n+++ b/xarray/core/dataset.py\n@@ -4177,7 +4177,7 @@ class Dataset(\n         }\n         variables.update(new_variables)\n \n-        coord_names = set(new_variables) | self._coord_names\n+        coord_names = (set(new_variables) | self._coord_names) - set(drop_variables)\n \n         return self._replace(variables, coord_names=coord_names, indexes=indexes)\n \n"}
{"instance_id": "mwaskom__seaborn-3187", "model_name_or_path": "devchain", "model_patch": "diff --git a/seaborn/_core/scales.py b/seaborn/_core/scales.py\nindex 55874bf2..29e79af5 100644\n--- a/seaborn/_core/scales.py\n+++ b/seaborn/_core/scales.py\n@@ -379,6 +379,16 @@ class ContinuousBase(Scale):\n             locs = axis.major.locator()\n             locs = locs[(vmin <= locs) & (locs <= vmax)]\n             labels = axis.major.formatter.format_ticks(locs)\n+            # When using ScalarFormatter, labels may be formatted with an offset\n+            # (e.g., \"3\" instead of \"3000000\" when offset is 1e6).\n+            # We need to re-format the labels without the offset to show full values.\n+            # Check for offset using get_offset() which is available on formatters\n+            # that compute offsets (ScalarFormatter, LogFormatterSciNotation).\n+            if hasattr(axis.major.formatter, \"get_offset\"):\n+                offset = axis.major.formatter.get_offset()\n+                if offset:\n+                    # Format the labels as full values without the offset or scientific notation\n+                    labels = [f\"{int(loc) if loc == int(loc) else loc}\" for loc in locs]\n             new._legend = list(locs), list(labels)\n \n         return new\n"}
{"instance_id": "sympy__sympy-13974", "model_name_or_path": "devchain", "model_patch": "diff --git a/sympy/physics/quantum/tensorproduct.py b/sympy/physics/quantum/tensorproduct.py\nindex 9dd10d219e..77f3e7cfc7 100644\n--- a/sympy/physics/quantum/tensorproduct.py\n+++ b/sympy/physics/quantum/tensorproduct.py\n@@ -316,7 +316,15 @@ def tensor_product_simp_Mul(e):\n         return e\n     c_part, nc_part = e.args_cnc()\n     n_nc = len(nc_part)\n-    if n_nc == 0 or n_nc == 1:\n+    if n_nc == 0:\n+        return e\n+    elif n_nc == 1:\n+        # Handle case where the single nc part is a Pow of TensorProduct\n+        # e.g., x**2 * (TensorProduct(1, A))**2 -> x**2 * TensorProduct(1, A**2)\n+        if isinstance(nc_part[0], Pow):\n+            nc_simp = tensor_product_simp(nc_part[0])\n+            if nc_simp != nc_part[0]:\n+                return Mul(*c_part) * nc_simp\n         return e\n     elif e.has(TensorProduct):\n         current = nc_part[0]\n@@ -382,7 +390,12 @@ def tensor_product_simp(e, **hints):\n     if isinstance(e, Add):\n         return Add(*[tensor_product_simp(arg) for arg in e.args])\n     elif isinstance(e, Pow):\n-        return tensor_product_simp(e.base) ** e.exp\n+        base = tensor_product_simp(e.base)\n+        if isinstance(base, TensorProduct):\n+            # Distribute exponent across tensor product elements\n+            # TensorProduct(a,b)**2 -> TensorProduct(a**2, b**2)\n+            return TensorProduct(*[arg**e.exp for arg in base.args])\n+        return base ** e.exp\n     elif isinstance(e, Mul):\n         return tensor_product_simp_Mul(e)\n     elif isinstance(e, Commutator):\n@@ -391,3 +404,19 @@ def tensor_product_simp(e, **hints):\n         return AntiCommutator(*[tensor_product_simp(arg) for arg in e.args])\n     else:\n         return e\n+\n+\n+# Monkey-patch Mul and Pow to support expand(tensorproduct=True)\n+def _Mul_eval_expand_tensorproduct(self, **hints):\n+    return tensor_product_simp(self)\n+\n+\n+def _Pow_eval_expand_tensorproduct(self, **hints):\n+    return tensor_product_simp(self)\n+\n+\n+# Add the methods only if they don't already exist\n+if not hasattr(Mul, '_eval_expand_tensorproduct'):\n+    Mul._eval_expand_tensorproduct = _Mul_eval_expand_tensorproduct\n+if not hasattr(Pow, '_eval_expand_tensorproduct'):\n+    Pow._eval_expand_tensorproduct = _Pow_eval_expand_tensorproduct\n"}
{"instance_id": "django__django-15127", "model_name_or_path": "devchain", "model_patch": "diff --git a/django/contrib/messages/storage/base.py b/django/contrib/messages/storage/base.py\nindex 01422066a6..a46c57cd53 100644\n--- a/django/contrib/messages/storage/base.py\n+++ b/django/contrib/messages/storage/base.py\n@@ -38,7 +38,7 @@ class Message:\n \n     @property\n     def level_tag(self):\n-        return LEVEL_TAGS.get(self.level, '')\n+        return utils.get_level_tags().get(self.level, '')\n \n \n class BaseStorage:\n"}
{"instance_id": "sphinx-doc__sphinx-8056", "model_name_or_path": "devchain", "model_patch": "diff --git a/sphinx/ext/napoleon/docstring.py b/sphinx/ext/napoleon/docstring.py\nindex b3daa06f1..ef9b5fc57 100644\n--- a/sphinx/ext/napoleon/docstring.py\n+++ b/sphinx/ext/napoleon/docstring.py\n@@ -273,7 +273,27 @@ class GoogleDocstring:\n         while not self._is_section_break():\n             _name, _type, _desc = self._consume_field(parse_type, prefer_type)\n             if _name or _type or _desc:\n-                fields.append((_name, _type, _desc,))\n+                # Check if multiple parameter names are separated by commas\n+                # Only split for simple names (no parentheses, backticks, or all asterisks)\n+                # This handles numpy-style parameters like \"x1, x2 : array_like\"\n+                # but doesn't split method signatures like \"func(i, j)\"\n+                # or special cases like \"*args, **kwargs\"\n+                names = [n.strip() for n in _name.split(',')]\n+                # Helper to check if a name is an asterisk parameter (escaped or not)\n+                def is_asterisk_param(n):\n+                    return n.startswith('*') or n.startswith(r'\\*')\n+                if (\n+                    len(names) > 1\n+                    and all(names)\n+                    and not any(('`' in n) for n in names)\n+                    and not any(('(' in n or ')' in n) for n in names)\n+                    and not all(is_asterisk_param(n) for n in names if n)\n+                ):\n+                    # Split into multiple fields with the same type and description\n+                    for name in names:\n+                        fields.append((name, _type, _desc,))\n+                else:\n+                    fields.append((_name, _type, _desc,))\n         return fields\n \n     def _consume_inline_attribute(self) -> Tuple[str, List[str]]:\ndiff --git a/tox.ini b/tox.ini\nindex bddd822a6..34baee205 100644\n--- a/tox.ini\n+++ b/tox.ini\n@@ -27,7 +27,7 @@ extras =\n setenv =\n     PYTHONWARNINGS = all,ignore::ImportWarning:importlib._bootstrap_external,ignore::DeprecationWarning:site,ignore::DeprecationWarning:distutils\n commands=\n-    pytest --durations 25 {posargs}\n+    pytest -rA --durations 25 {posargs}\n \n [testenv:flake8]\n basepython = python3\n"}
{"instance_id": "matplotlib__matplotlib-24870", "model_name_or_path": "devchain", "model_patch": "diff --git a/lib/matplotlib/contour.py b/lib/matplotlib/contour.py\nindex 144eadeae2..a64486d7aa 100644\n--- a/lib/matplotlib/contour.py\n+++ b/lib/matplotlib/contour.py\n@@ -1446,15 +1446,39 @@ class QuadContourSet(ContourSet):\n         else:\n             fn = 'contour'\n         nargs = len(args)\n+        # Auto-detect boolean arrays and set default levels\n+        # Check if input is boolean and no explicit levels provided\n+        levels_provided = (self.levels is not None)\n         if nargs <= 2:\n+            # Check if the input array is boolean before conversion\n+            z_input = np.asarray(args[0])\n+            z_is_bool = (z_input.dtype == bool)\n             z = ma.asarray(args[0], dtype=np.float64)\n             x, y = self._initialize_x_y(z)\n             args = args[1:]\n+            if not levels_provided:\n+                levels_provided = (len(args) > 0 and args[0] is not None)\n         elif nargs <= 4:\n+            # Check if Z is boolean before conversion\n+            z_input = np.asarray(args[2])\n+            z_is_bool = (z_input.dtype == bool)\n             x, y, z = self._check_xyz(args[:3], kwargs)\n             args = args[3:]\n+            if not levels_provided:\n+                levels_provided = (len(args) > 0 and args[0] is not None)\n         else:\n             raise _api.nargs_error(fn, takes=\"from 1 to 4\", given=nargs)\n+\n+        # For boolean arrays with no explicit levels, set appropriate defaults\n+        if z_is_bool and not levels_provided:\n+            if self.filled:\n+                # Filled contours need at least 2 levels. Use [0.0, 1.0] to define\n+                # the boundary between False and True regions.\n+                self.levels = [0.0, 1.0]\n+            else:\n+                # Line contours: single level at 0.5 to show the boundary.\n+                self.levels = [0.5]\n+\n         z = ma.masked_invalid(z, copy=False)\n         self.zmax = float(z.max())\n         self.zmin = float(z.min())\n"}
{"instance_id": "django__django-13297", "model_name_or_path": "devchain", "model_patch": "diff --git a/django/db/backends/sqlite3/base.py b/django/db/backends/sqlite3/base.py\nindex 8a105d4f35..b2c462d749 100644\n--- a/django/db/backends/sqlite3/base.py\n+++ b/django/db/backends/sqlite3/base.py\n@@ -76,6 +76,20 @@ Database.register_converter(\"timestamp\", decoder(parse_datetime))\n \n Database.register_adapter(decimal.Decimal, str)\n \n+def adapt_simple_lazy_object(val):\n+    \"\"\"\n+    Adapt SimpleLazyObject to a value SQLite can handle. Force evaluation\n+    and return the string representation.\n+    \"\"\"\n+    # Force evaluation by calling str() on the lazy object.\n+    # This triggers the _setup() method and returns the actual value.\n+    return str(val)\n+\n+\n+# Import and register adapter for SimpleLazyObject\n+from django.utils.functional import SimpleLazyObject\n+Database.register_adapter(SimpleLazyObject, adapt_simple_lazy_object)\n+\n \n class DatabaseWrapper(BaseDatabaseWrapper):\n     vendor = 'sqlite'\ndiff --git a/django/utils/functional.py b/django/utils/functional.py\nindex 6d38f932f9..3916b9995d 100644\n--- a/django/utils/functional.py\n+++ b/django/utils/functional.py\n@@ -403,6 +403,15 @@ class SimpleLazyObject(LazyObject):\n         return copy.deepcopy(self._wrapped, memo)\n \n \n+    # Provide a default __reduce_ex__ to ensure the wrapped object is returned\n+    # when pickling. This is needed for database adapters that use pickle-like\n+    # protocols to determine how to handle objects (e.g., SQLite parameter\n+    # binding uses the reduce protocol).\n+    def __reduce_ex__(self, protocol):\n+        if self._wrapped is empty:\n+            self._setup()\n+        return self._wrapped.__reduce_ex__(protocol)\n+\n def partition(predicate, values):\n     \"\"\"\n     Split the values into two sets, based on the return value of the function\n"}
{"instance_id": "sympy__sympy-13852", "model_name_or_path": "devchain", "model_patch": "diff --git a/sympy/functions/special/zeta_functions.py b/sympy/functions/special/zeta_functions.py\nindex b8e2b66195..e376b97e80 100644\n--- a/sympy/functions/special/zeta_functions.py\n+++ b/sympy/functions/special/zeta_functions.py\n@@ -277,6 +277,9 @@ def eval(cls, s, z):\n             return -dirichlet_eta(s)\n         elif z == 0:\n             return 0\n+        # polylog(2, 1/2) = pi^2/12 - log(2)^2/2\n+        if s == 2 and z == S(1)/2:\n+            return pi**2/12 - log(2)**2/2\n \n     def fdiff(self, argindex=1):\n         s, z = self.args\n@@ -291,7 +294,7 @@ def _eval_expand_func(self, **hints):\n         from sympy import log, expand_mul, Dummy, exp_polar, I\n         s, z = self.args\n         if s == 1:\n-            return -log(1 + exp_polar(-I*pi)*z)\n+            return -log(1 - z)\n         if s.is_Integer and s <= 0:\n             u = Dummy('u')\n             start = u/(1 - u)\n"}
{"instance_id": "django__django-16950", "model_name_or_path": "devchain", "model_patch": "diff --git a/django/forms/models.py b/django/forms/models.py\nindex 3fa04b821f..7ca084e974 100644\n--- a/django/forms/models.py\n+++ b/django/forms/models.py\n@@ -1177,7 +1177,12 @@ class BaseInlineFormSet(BaseModelFormSet):\n                 to_field = self.instance._meta.get_field(kwargs[\"to_field\"])\n             else:\n                 to_field = self.instance._meta.pk\n-            if to_field.has_default():\n+            if to_field.has_default() and (\n+                # Don't ignore a parent's auto-generated key if it's not the\n+                # parent model's pk and form data is provided.\n+                to_field.attname == self.fk.remote_field.model._meta.pk.name\n+                or not form.data\n+            ):\n                 setattr(self.instance, to_field.attname, None)\n \n         form.fields[name] = InlineForeignKeyField(self.instance, **kwargs)\n"}
{"instance_id": "sympy__sympy-22080", "model_name_or_path": "devchain", "model_patch": "diff --git a/sympy/printing/pycode.py b/sympy/printing/pycode.py\nindex b669a2c783..b6578c66a4 100644\n--- a/sympy/printing/pycode.py\n+++ b/sympy/printing/pycode.py\n@@ -234,7 +234,13 @@ def _print_ComplexInfinity(self, expr):\n \n     def _print_Mod(self, expr):\n         PREC = precedence(expr)\n-        return ('{} % {}'.format(*map(lambda x: self.parenthesize(x, PREC), expr.args)))\n+        result = '{} % {}'.format(*map(lambda x: self.parenthesize(x, PREC), expr.args))\n+        # In Python, the % operator has lower precedence than many other operators,\n+        # including unary -. To ensure correct evaluation, always wrap the Mod\n+        # expression in parentheses when it could be ambiguous. The precedence of\n+        # the generated % operator is the same as *, but SymPy's internal Mod\n+        # precedence is higher (Func), so we need parentheses to preserve semantics.\n+        return '({})'.format(result)\n \n     def _print_Piecewise(self, expr):\n         result = []\n"}
{"instance_id": "pytest-dev__pytest-5840", "model_name_or_path": "devchain", "model_patch": "diff --git a/src/_pytest/config/__init__.py b/src/_pytest/config/__init__.py\nindex 3a0eca546..b4454d809 100644\n--- a/src/_pytest/config/__init__.py\n+++ b/src/_pytest/config/__init__.py\n@@ -367,7 +367,7 @@ class PytestPluginManager(PluginManager):\n         \"\"\"\n         current = py.path.local()\n         self._confcutdir = (\n-            unique_path(current.join(namespace.confcutdir, abs=True))\n+            current.join(namespace.confcutdir, abs=True)\n             if namespace.confcutdir\n             else None\n         )\n@@ -406,8 +406,6 @@ class PytestPluginManager(PluginManager):\n         else:\n             directory = path\n \n-        directory = unique_path(directory)\n-\n         # XXX these days we may rather want to use config.rootdir\n         # and allow users to opt into looking into the rootdir parent\n         # directories instead of requiring to specify confcutdir\n@@ -432,12 +430,14 @@ class PytestPluginManager(PluginManager):\n         raise KeyError(name)\n \n     def _importconftest(self, conftestpath):\n-        # Use realpath to avoid loading the same conftest twice\n+        # Use a resolved Path object as key to avoid loading the same conftest twice\n         # with build systems that create build directories containing\n         # symlinks to actual files.\n-        conftestpath = unique_path(conftestpath)\n+        # Using Path().resolve() is better than py.path.realpath because\n+        # it resolves to the correct path/drive in case-insensitive file systems (#5792)\n+        key = Path(str(conftestpath)).resolve()\n         try:\n-            return self._conftestpath2mod[conftestpath]\n+            return self._conftestpath2mod[key]\n         except KeyError:\n             pkgpath = conftestpath.pypkgpath()\n             if pkgpath is None:\n@@ -454,7 +454,7 @@ class PytestPluginManager(PluginManager):\n                 raise ConftestImportFailure(conftestpath, sys.exc_info())\n \n             self._conftest_plugins.add(mod)\n-            self._conftestpath2mod[conftestpath] = mod\n+            self._conftestpath2mod[key] = mod\n             dirpath = conftestpath.dirpath()\n             if dirpath in self._dirpath2confmods:\n                 for path, mods in self._dirpath2confmods.items():\ndiff --git a/src/_pytest/pathlib.py b/src/_pytest/pathlib.py\nindex 0403b6947..45f3c22b0 100644\n--- a/src/_pytest/pathlib.py\n+++ b/src/_pytest/pathlib.py\n@@ -338,9 +338,10 @@ def parts(s):\n \n \n def unique_path(path):\n-    \"\"\"Returns a unique path in case-insensitive (but case-preserving) file\n-    systems such as Windows.\n+    \"\"\"Returns a unique path for caching purposes.\n \n-    This is needed only for ``py.path.local``; ``pathlib.Path`` handles this\n-    natively with ``resolve()``.\"\"\"\n-    return type(path)(normcase(str(path.realpath())))\n+    This function now uses Path().resolve() which is better than\n+    normcase() for case-insensitive file systems because it preserves\n+    the correct case for module imports.\n+    \"\"\"\n+    return Path(str(path.realpath()))\n"}
{"instance_id": "pylint-dev__pylint-8898", "model_name_or_path": "devchain", "model_patch": "diff --git a/pylint/config/utils.py b/pylint/config/utils.py\nindex 91e4ff86f..4dfb28996 100644\n--- a/pylint/config/utils.py\n+++ b/pylint/config/utils.py\n@@ -6,6 +6,8 @@\n \n from __future__ import annotations\n \n+import csv\n+import io\n import re\n from collections.abc import Callable, Sequence\n from pathlib import Path\n@@ -132,9 +134,18 @@ def _convert_option_to_argument(\n \n \n def _parse_rich_type_value(value: Any) -> str:\n-    \"\"\"Parse rich (toml) types into strings.\"\"\"\n+    \"\"\"Parse rich (toml) types into strings.\n+\n+    For list/tuple types, we use csv formatting to properly escape/quote\n+    elements that contain commas (e.g., regex patterns like \"(foo{1,3})\").\n+    \"\"\"\n     if isinstance(value, (list, tuple)):\n-        return \",\".join(_parse_rich_type_value(i) for i in value)\n+        # Use csv module to properly quote/escape values containing commas\n+        output = io.StringIO()\n+        writer = csv.writer(output, quoting=csv.QUOTE_MINIMAL)\n+        parsed_values = [_parse_rich_type_value(i) for i in value]\n+        writer.writerow(parsed_values)\n+        return output.getvalue().rstrip(\"\\n\\r\")\n     if isinstance(value, re.Pattern):\n         return str(value.pattern)\n     if isinstance(value, dict):\ndiff --git a/pylint/utils/utils.py b/pylint/utils/utils.py\nindex fa7f7b92f..671f9474c 100644\n--- a/pylint/utils/utils.py\n+++ b/pylint/utils/utils.py\n@@ -16,6 +16,7 @@ except ImportError:  # isort < 5\n \n import argparse\n import codecs\n+import csv\n import os\n import re\n import sys\n@@ -213,12 +214,23 @@ def _splitstrip(string: str, sep: str = \",\") -> list[str]:\n     \"\"\"Return a list of stripped string by splitting the string given as\n     argument on `sep` (',' by default), empty strings are discarded.\n \n+    Handles quoted strings properly (commas inside quotes are not separators).\n+    Also handles commas inside regex quantifiers like {1,3}.\n+\n     >>> _splitstrip('a, b, c   ,  4,,')\n     ['a', 'b', 'c', '4']\n     >>> _splitstrip('a')\n     ['a']\n-    >>> _splitstrip('a,\\nb,\\nc,')\n+    >>> _splitstrip('a,\\\\nb,\\\\nc,')\n     ['a', 'b', 'c']\n+    >>> _splitstrip('\"foo{1,3}\"')\n+    ['foo{1,3}']\n+    >>> _splitstrip('\"foo{1,3}\",bar')\n+    ['foo{1,3}', 'bar']\n+    >>> _splitstrip('foo,\"bar,baz\",qux')\n+    ['foo', 'bar,baz', 'qux']\n+    >>> _splitstrip('(foo{1,3})')\n+    ['(foo{1,3})']\n \n     :type string: str or unicode\n     :param string: a csv line\n@@ -229,7 +241,79 @@ def _splitstrip(string: str, sep: str = \",\") -> list[str]:\n     :rtype: str or unicode\n     :return: the unquoted string (or the input string if it wasn't quoted)\n     \"\"\"\n-    return [word.strip() for word in string.split(sep) if word.strip()]\n+    # Skip empty strings\n+    if not string or not string.strip():\n+        return []\n+\n+    # First, try using csv module to handle quoted strings\n+    try:\n+        reader = csv.reader([string], skipinitialspace=True)\n+        result = next(reader)\n+    except csv.Error:\n+        # CSV parsing failed (e.g., embedded newlines), fall through to manual handling\n+        result = None\n+\n+    if result is not None:\n+        # Check if any result looks like a broken regex (comma inside {})\n+        # This handles cases like \"(foo{1,3})\" where TOML stripped the quotes\n+        has_broken_regex = False\n+        for item in result:\n+            # Check if item has unclosed braces or brackets\n+            # which would indicate we split inside a regex quantifier\n+            if item.count('{') > item.count('}') or item.count('[') > item.count(']'):\n+                has_broken_regex = True\n+                break\n+\n+        if not has_broken_regex:\n+            # CSV parsing worked and didn't break regex patterns\n+            return [_unquote(item.strip()) for item in result if item.strip()]\n+\n+    # Fallback: Manual split that respects braces/brackets\n+    # This handles commas inside regex quantifiers like {1,3}\n+    result = []\n+    current = []\n+    brace_depth = 0\n+    bracket_depth = 0\n+    in_quotes = False\n+    quote_char = None\n+\n+    for char in string:\n+        if not in_quotes and char in \"\\\"'\":\n+            in_quotes = True\n+            quote_char = char\n+            current.append(char)\n+        elif in_quotes and char == quote_char:\n+            in_quotes = False\n+            quote_char = None\n+            current.append(char)\n+        elif not in_quotes:\n+            if char == '{':\n+                brace_depth += 1\n+                current.append(char)\n+            elif char == '}':\n+                brace_depth = max(0, brace_depth - 1)\n+                current.append(char)\n+            elif char == '[':\n+                bracket_depth += 1\n+                current.append(char)\n+            elif char == ']':\n+                bracket_depth = max(0, bracket_depth - 1)\n+                current.append(char)\n+            elif char == sep and brace_depth == 0 and bracket_depth == 0:\n+                # Split point found outside braces/brackets\n+                result.append(''.join(current))\n+                current = []\n+            else:\n+                current.append(char)\n+        else:\n+            current.append(char)\n+\n+    # Add the last segment\n+    if current:\n+        result.append(''.join(current))\n+\n+    # Strip whitespace, unquote, and filter empty strings\n+    return [_unquote(item.strip()) for item in result if item.strip()]\n \n \n def _unquote(string: str) -> str:\ndiff --git a/test_verify_fix.py b/test_verify_fix.py\nnew file mode 100644\nindex 000000000..e93c22706\n--- /dev/null\n+++ b/test_verify_fix.py\n@@ -0,0 +1,73 @@\n+#!/usr/bin/env python3\n+\"\"\"Verify the fix for bad-names-rgxs issue.\"\"\"\n+\n+from pylint.utils.utils import _splitstrip\n+from pylint.config.utils import _parse_rich_type_value\n+\n+print('=== Testing the fix for bad-names-rgxs ===')\n+print()\n+\n+# Test 1: The original repro case - single regex with comma (unquoted, as from TOML)\n+print('Test 1: Single regex with comma (unquoted, as from TOML parsing)')\n+result = _splitstrip('(foo{1,3})')\n+print(f'  Input: (foo{{1,3}})')\n+print(f'  Result: {result}')\n+print(f'  Expected: [\"(foo{{1,3}})\"]')\n+print(f'  PASS' if result == ['(foo{1,3})'] else '  FAIL')\n+print()\n+\n+# Test 2: TOML list format\n+print('Test 2: TOML list format')\n+value_list = ['(foo{1,3})']\n+joined = _parse_rich_type_value(value_list)\n+result = _splitstrip(joined)\n+print(f'  Input list: {value_list}')\n+print(f'  After _parse_rich_type_value: {repr(joined)}')\n+print(f'  After _splitstrip: {result}')\n+print(f'  Expected: [\"(foo{1,3})\"]')\n+print(f'  PASS' if result == ['(foo{1,3})'] else '  FAIL')\n+print()\n+\n+# Test 3: Quoted string\n+print('Test 3: Quoted string')\n+result = _splitstrip('\"(foo{1,3})\"')\n+print(f'  Input: \"(foo{{1,3}})\"')\n+print(f'  Result: {result}')\n+print(f'  Expected: [\"(foo{1,3})\"]')\n+print(f'  PASS' if result == ['(foo{1,3})'] else '  FAIL')\n+print()\n+\n+# Test 4: Multiple values with comma inside one value\n+print('Test 4: Multiple values with comma inside one value')\n+result = _splitstrip('\"(foo{1,3})\",bar.*')\n+print(f'  Input: \"(foo{{1,3}})\",bar.*')\n+print(f'  Result: {result}')\n+print(f'  Expected: [\"(foo{1,3})\", \"bar.*\"]')\n+print(f'  PASS' if result == ['(foo{1,3})', 'bar.*'] else '  FAIL')\n+print()\n+\n+# Test 5: Backward compatibility - normal CSV\n+print('Test 5: Backward compatibility - normal CSV')\n+result = _splitstrip('foo.*,bar.*,baz.*')\n+print(f'  Input: foo.*,bar.*,baz.*')\n+print(f'  Result: {result}')\n+print(f'  Expected: [\"foo.*\", \"bar.*\", \"baz.*\"]')\n+print(f'  PASS' if result == ['foo.*', 'bar.*', 'baz.*'] else '  FAIL')\n+print()\n+\n+# Test 6: Complex case with multiple braces\n+print('Test 6: Complex case with nested quantifiers')\n+result = _splitstrip('foo{1,3},bar{2,5}')\n+print(f'  Input: foo{{1,3}},bar{{2,5}}')\n+print(f'  Result: {result}')\n+print(f'  Expected: [\"foo{{1,3}}\", \"bar{{2,5}}\"]')\n+print(f'  PASS' if result == ['foo{1,3}', 'bar{2,5}'] else '  FAIL')\n+print()\n+\n+# Test 7: Invalid regex should still be caught\n+print('Test 7: Invalid regex (unclosed bracket) should still error')\n+result = _splitstrip('(foo[bar')\n+print(f'  Input: (foo[bar')\n+print(f'  Result: {result}')\n+print(f'  Expected: [\"(foo[bar\"]')\n+print(f'  PASS' if result == ['(foo[bar]'] else '  FAIL')\n"}
{"instance_id": "django__django-11400", "model_name_or_path": "devchain", "model_patch": "diff --git a/django/contrib/admin/filters.py b/django/contrib/admin/filters.py\nindex d65e01d5e2..838e7745db 100644\n--- a/django/contrib/admin/filters.py\n+++ b/django/contrib/admin/filters.py\n@@ -198,6 +198,8 @@ class RelatedFieldListFilter(FieldListFilter):\n         related_admin = model_admin.admin_site._registry.get(field.remote_field.model)\n         if related_admin is not None:\n             ordering = related_admin.get_ordering(request)\n+        if not ordering:\n+            ordering = field.remote_field.model._meta.ordering\n         return field.get_choices(include_blank=False, ordering=ordering)\n \n     def choices(self, changelist):\n@@ -418,5 +420,14 @@ FieldListFilter.register(lambda f: True, AllValuesFieldListFilter)\n \n class RelatedOnlyFieldListFilter(RelatedFieldListFilter):\n     def field_choices(self, field, request, model_admin):\n+        ordering = ()\n+        related_admin = model_admin.admin_site._registry.get(field.remote_field.model)\n+        if related_admin is not None:\n+            ordering = related_admin.get_ordering(request)\n+        if not ordering:\n+            ordering = field.remote_field.model._meta.ordering\n         pk_qs = model_admin.get_queryset(request).distinct().values_list('%s__pk' % self.field_path, flat=True)\n-        return field.get_choices(include_blank=False, limit_choices_to={'pk__in': pk_qs})\n+        return field.get_choices(\n+            include_blank=False, ordering=ordering,\n+            limit_choices_to={'pk__in': pk_qs}\n+        )\n"}
{"instance_id": "astropy__astropy-13398", "model_name_or_path": "devchain", "model_patch": "diff --git a/astropy/coordinates/builtin_frames/intermediate_rotation_transforms.py b/astropy/coordinates/builtin_frames/intermediate_rotation_transforms.py\nindex a15248c38d..acd27a0891 100644\n--- a/astropy/coordinates/builtin_frames/intermediate_rotation_transforms.py\n+++ b/astropy/coordinates/builtin_frames/intermediate_rotation_transforms.py\n@@ -8,16 +8,19 @@ rotations without aberration corrections or offsets.\n import numpy as np\n import erfa\n \n+from astropy import units as u\n from astropy.coordinates.baseframe import frame_transform_graph\n from astropy.coordinates.transformations import FunctionTransformWithFiniteDifference\n-from astropy.coordinates.matrix_utilities import matrix_transpose\n+from astropy.coordinates.matrix_utilities import rotation_matrix, matrix_transpose\n \n from .icrs import ICRS\n from .gcrs import GCRS, PrecessedGeocentric\n from .cirs import CIRS\n from .itrs import ITRS\n from .equatorial import TEME, TETE\n-from .utils import get_polar_motion, get_jd12, EARTH_CENTER\n+from .altaz import AltAz\n+from .hadec import HADec\n+from .utils import get_polar_motion, get_jd12, EARTH_CENTER, PIOVER2\n \n # # first define helper functions\n \n@@ -272,6 +275,90 @@ def itrs_to_teme(itrs_coo, teme_frame):\n     return teme_frame.realize_frame(newrepr)\n \n \n+# ITRS to Observed (AltAz/HADec) transforms that stay within ITRS\n+# These transforms treat ITRS coordinates as time invariant, avoiding the\n+# problematic ITRS->ITRS transform that references to the SSB\n+\n+\n+def itrs_to_observed_mat(observed_frame):\n+    \"\"\"\n+    Create the transformation matrix from ITRS to observed frame (AltAz or HADec).\n+\n+    Parameters\n+    ----------\n+    observed_frame : AltAz or HADec\n+        The target observed frame.\n+\n+    Returns\n+    -------\n+    mat : ndarray\n+        The 3x3 transformation matrix.\n+    \"\"\"\n+    lon, lat, height = observed_frame.location.to_geodetic('WGS84')\n+    elong = lon.to_value(u.radian)\n+\n+    if isinstance(observed_frame, AltAz):\n+        # form ITRS to AltAz matrix\n+        elat = lat.to_value(u.radian)\n+        # AltAz frame is left handed\n+        minus_x = np.eye(3)\n+        minus_x[0][0] = -1.0\n+        mat = (minus_x\n+               @ rotation_matrix(PIOVER2 - elat, 'y', unit=u.radian)\n+               @ rotation_matrix(elong, 'z', unit=u.radian))\n+\n+    else:  # HADec\n+        # form ITRS to HADec matrix\n+        # HADec frame is left handed\n+        minus_y = np.eye(3)\n+        minus_y[1][1] = -1.0\n+        mat = (minus_y\n+               @ rotation_matrix(elong, 'z', unit=u.radian))\n+    return mat\n+\n+\n+@frame_transform_graph.transform(FunctionTransformWithFiniteDifference, ITRS, AltAz)\n+@frame_transform_graph.transform(FunctionTransformWithFiniteDifference, ITRS, HADec)\n+def itrs_to_observed(itrs_coo, observed_frame):\n+    \"\"\"\n+    Transform from ITRS to observed frame (AltAz or HADec).\n+\n+    This transform treats ITRS coordinates as time invariant, avoiding the\n+    problematic ITRS->ITRS transform that references to the SSB. Instead,\n+    it simply adopts the obstime of the output frame.\n+\n+    This approach is appropriate for nearby objects (satellites, airplanes, etc.)\n+    where geocentric aberration is not relevant.\n+    \"\"\"\n+    # Trying to synchronize the obstimes here makes no sense. In fact,\n+    # it's a real gotcha as doing an ITRS->ITRS transform references\n+    # ITRS coordinates, which should be tied to the Earth, to the SSB.\n+    # Instead, we treat ITRS coordinates as time invariant here.\n+\n+    # form the Topocentric ITRS position\n+    topocentric_itrs_repr = (itrs_coo.cartesian\n+                             - observed_frame.location.get_itrs().cartesian)\n+    rep = topocentric_itrs_repr.transform(itrs_to_observed_mat(observed_frame))\n+    return observed_frame.realize_frame(rep)\n+\n+\n+@frame_transform_graph.transform(FunctionTransformWithFiniteDifference, AltAz, ITRS)\n+@frame_transform_graph.transform(FunctionTransformWithFiniteDifference, HADec, ITRS)\n+def observed_to_itrs(observed_coo, itrs_frame):\n+    \"\"\"\n+    Transform from observed frame (AltAz or HADec) to ITRS.\n+\n+    This is the inverse of itrs_to_observed, treating ITRS coordinates\n+    as time invariant.\n+    \"\"\"\n+    # form the Topocentric ITRS position\n+    topocentric_itrs_repr = observed_coo.cartesian.transform(matrix_transpose(\n+                            itrs_to_observed_mat(observed_coo)))\n+    # form the Geocentric ITRS position\n+    rep = topocentric_itrs_repr + observed_coo.location.get_itrs().cartesian\n+    return itrs_frame.realize_frame(rep)\n+\n+\n # Create loopback transformations\n frame_transform_graph._add_merged_transform(ITRS, CIRS, ITRS)\n frame_transform_graph._add_merged_transform(PrecessedGeocentric, GCRS, PrecessedGeocentric)\n"}
{"instance_id": "django__django-11206", "model_name_or_path": "devchain", "model_patch": "diff --git a/django/utils/numberformat.py b/django/utils/numberformat.py\nindex 88b35fc435..c6f6113b6a 100644\n--- a/django/utils/numberformat.py\n+++ b/django/utils/numberformat.py\n@@ -31,14 +31,20 @@ def format(number, decimal_sep, decimal_pos=None, grouping=0, thousand_sep='',\n         # scientific notation to avoid high memory usage in {:f}'.format().\n         _, digits, exponent = number.as_tuple()\n         if abs(exponent) + len(digits) > 200:\n-            number = '{:e}'.format(number)\n-            coefficient, exponent = number.split('e')\n-            # Format the coefficient.\n-            coefficient = format(\n-                coefficient, decimal_sep, decimal_pos, grouping,\n-                thousand_sep, force_grouping, use_l10n,\n-            )\n-            return '{}e{}'.format(coefficient, exponent)\n+            # When decimal_pos is specified and the number is too small to be\n+            # represented, return zero instead of exponential notation.\n+            if decimal_pos is not None and exponent < 0 and exponent + len(digits) <= -decimal_pos:\n+                # The number is smaller than can be represented with decimal_pos\n+                str_number = '0'\n+            else:\n+                number = '{:e}'.format(number)\n+                coefficient, exponent = number.split('e')\n+                # Format the coefficient.\n+                coefficient = format(\n+                    coefficient, decimal_sep, decimal_pos, grouping,\n+                    thousand_sep, force_grouping, use_l10n,\n+                )\n+                return '{}e{}'.format(coefficient, exponent)\n         else:\n             str_number = '{:f}'.format(number)\n     else:\n"}
{"instance_id": "django__django-11433", "model_name_or_path": "devchain", "model_patch": "diff --git a/django/forms/models.py b/django/forms/models.py\nindex 5edbbd376f..0c4859572e 100644\n--- a/django/forms/models.py\n+++ b/django/forms/models.py\n@@ -48,9 +48,21 @@ def construct_instance(form, instance, fields=None, exclude=None):\n             continue\n         # Leave defaults for fields that aren't in POST data, except for\n         # checkbox inputs because they don't appear in POST data if not checked.\n-        if (f.has_default() and\n-                form[f.name].field.widget.value_omitted_from_data(form.data, form.files, form.add_prefix(f.name))):\n-            continue\n+        # If the field is in cleaned_data but was not in POST data, it was set\n+        # programmatically (e.g., in form.clean()) and should override the default.\n+        prefixed_name = form.add_prefix(f.name)\n+        value_omitted = form[f.name].field.widget.value_omitted_from_data(\n+            form.data, form.files, prefixed_name\n+        )\n+        if f.has_default() and value_omitted:\n+            # The value was omitted from POST data. Check if it was set\n+            # programmatically in clean() by comparing to the field's empty value.\n+            field = form[f.name].field\n+            if cleaned_data[f.name] in field.empty_values:\n+                # Field wasn't submitted and wasn't set programmatically,\n+                # use the model default.\n+                continue\n+            # Otherwise, the field was set programmatically, use cleaned_data.\n         # Defer saving file-type fields until after the other fields, so a\n         # callable upload_to can use the values from other fields.\n         if isinstance(f, models.FileField):\n"}
{"instance_id": "scikit-learn__scikit-learn-26194", "model_name_or_path": "devchain", "model_patch": "diff --git a/sklearn/metrics/_ranking.py b/sklearn/metrics/_ranking.py\nindex 0cdead923..84dfd2f73 100644\n--- a/sklearn/metrics/_ranking.py\n+++ b/sklearn/metrics/_ranking.py\n@@ -1083,7 +1083,11 @@ def roc_curve(\n     # to make sure that the curve starts at (0, 0)\n     tps = np.r_[0, tps]\n     fps = np.r_[0, fps]\n-    thresholds = np.r_[thresholds[0] + 1, thresholds]\n+    # Use inf as the first threshold to ensure the curve starts at (0, 0).\n+    # This works for both probability estimates (0-1 range) and decision\n+    # function outputs (any real number), avoiding the issue of thresholds\n+    # exceeding 1 when using probability estimates.\n+    thresholds = np.r_[np.inf, thresholds]\n \n     if fps[-1] <= 0:\n         warnings.warn(\n"}
{"instance_id": "astropy__astropy-14182", "model_name_or_path": "devchain", "model_patch": "diff --git a/astropy/io/ascii/rst.py b/astropy/io/ascii/rst.py\nindex abdeba8efb..b7a087bfc1 100644\n--- a/astropy/io/ascii/rst.py\n+++ b/astropy/io/ascii/rst.py\n@@ -57,10 +57,11 @@ class RST(FixedWidth):\n     data_class = SimpleRSTData\n     header_class = SimpleRSTHeader\n \n-    def __init__(self):\n-        super().__init__(delimiter_pad=None, bookend=False)\n+    def __init__(self, **kwargs):\n+        super().__init__(delimiter_pad=None, bookend=False, **kwargs)\n \n     def write(self, lines):\n         lines = super().write(lines)\n-        lines = [lines[1]] + lines + [lines[1]]\n+        idx = len(self.data.header_rows)\n+        lines = [lines[idx]] + lines + [lines[idx]]\n         return lines\n"}
{"instance_id": "sphinx-doc__sphinx-7985", "model_name_or_path": "devchain", "model_patch": "diff --git a/sphinx/builders/linkcheck.py b/sphinx/builders/linkcheck.py\nindex dd5317087..c6cba2c9e 100644\n--- a/sphinx/builders/linkcheck.py\n+++ b/sphinx/builders/linkcheck.py\n@@ -116,6 +116,68 @@ class CheckExternalLinksBuilder(Builder):\n         if self.app.config.linkcheck_timeout:\n             kwargs['timeout'] = self.app.config.linkcheck_timeout\n \n+        def check_local_reference(uri: str, docname: str) -> Tuple[str, str, int]:\n+            \"\"\"Check if a local/internal reference exists.\n+\n+            Returns (status, info, code) tuple where status is one of:\n+            - 'working': reference exists\n+            - 'broken': reference does not exist\n+            \"\"\"\n+            # Get the standard domain which contains all reference labels\n+            std_domain = self.env.get_domain('std')\n+\n+            # Handle anchor-only references (same document)\n+            if uri.startswith('#'):\n+                anchor = uri[1:]\n+                doctree = self.env.get_doctree(docname)\n+                if anchor in doctree.ids:\n+                    return 'working', '', 0\n+                else:\n+                    return 'broken', __('anchor not found: %s') % anchor, 0\n+\n+            # Handle references to labels (same-document or cross-document)\n+            # These are simple names like 'local-link' or 'some-section'\n+            # Check in both named labels and anonymous labels\n+            if uri in std_domain.labels:\n+                # Reference exists - it's a named label with a title\n+                return 'working', '', 0\n+            elif uri in std_domain.anonlabels:\n+                # Reference exists - it's an anonymous label\n+                return 'working', '', 0\n+            elif uri in self.env.all_docs:\n+                # Reference exists - it's a document name\n+                return 'working', '', 0\n+            else:\n+                # Reference might be a combination of docname#anchor or other formats\n+                # Try to parse it\n+                if '#' in uri:\n+                    doc_ref, anchor = uri.split('#', 1)\n+                    # Check if it's docname#anchor format\n+                    if doc_ref in self.env.all_docs:\n+                        doctree = self.env.get_doctree(doc_ref)\n+                        if anchor in doctree.ids:\n+                            return 'working', '', 0\n+                        else:\n+                            return 'broken', __('anchor %r not found in %r') % (anchor, doc_ref), 0\n+                    # Check if it's a label#anchor format (label in a document)\n+                    elif doc_ref in std_domain.labels:\n+                        target_docname, labelid, _ = std_domain.labels[doc_ref]\n+                        doctree = self.env.get_doctree(target_docname)\n+                        if anchor in doctree.ids:\n+                            return 'working', '', 0\n+                        else:\n+                            return 'broken', __('anchor %r not found in %r') % (anchor, target_docname), 0\n+                    elif doc_ref in std_domain.anonlabels:\n+                        target_docname, labelid = std_domain.anonlabels[doc_ref]\n+                        doctree = self.env.get_doctree(target_docname)\n+                        if anchor in doctree.ids:\n+                            return 'working', '', 0\n+                        else:\n+                            return 'broken', __('anchor %r not found in %r') % (anchor, target_docname), 0\n+\n+                # Reference not found\n+                return 'broken', __('local reference not found: %s') % uri, 0\n+\n         def get_request_headers() -> Dict:\n             url = urlparse(uri)\n             candidates = [\"%s://%s\" % (url.scheme, url.netloc),\n@@ -210,10 +272,11 @@ class CheckExternalLinksBuilder(Builder):\n \n         def check() -> Tuple[str, str, int]:\n             # check for various conditions without bothering the network\n-            if len(uri) == 0 or uri.startswith(('#', 'mailto:', 'ftp:')):\n+            if len(uri) == 0 or uri.startswith(('mailto:', 'ftp:')):\n                 return 'unchecked', '', 0\n             elif not uri.startswith(('http:', 'https:')):\n-                return 'local', '', 0\n+                # Check local/internal references\n+                return check_local_reference(uri, docname)\n             elif uri in self.good:\n                 return 'working', 'old', 0\n             elif uri in self.broken:\ndiff --git a/tox.ini b/tox.ini\nindex bddd822a6..34baee205 100644\n--- a/tox.ini\n+++ b/tox.ini\n@@ -27,7 +27,7 @@ extras =\n setenv =\n     PYTHONWARNINGS = all,ignore::ImportWarning:importlib._bootstrap_external,ignore::DeprecationWarning:site,ignore::DeprecationWarning:distutils\n commands=\n-    pytest --durations 25 {posargs}\n+    pytest -rA --durations 25 {posargs}\n \n [testenv:flake8]\n basepython = python3\n"}
{"instance_id": "django__django-14170", "model_name_or_path": "devchain", "model_patch": "diff --git a/django/db/models/functions/datetime.py b/django/db/models/functions/datetime.py\nindex 90e6f41be0..bbe9fa8c79 100644\n--- a/django/db/models/functions/datetime.py\n+++ b/django/db/models/functions/datetime.py\n@@ -164,12 +164,6 @@ ExtractYear.register_lookup(YearGte)\n ExtractYear.register_lookup(YearLt)\n ExtractYear.register_lookup(YearLte)\n \n-ExtractIsoYear.register_lookup(YearExact)\n-ExtractIsoYear.register_lookup(YearGt)\n-ExtractIsoYear.register_lookup(YearGte)\n-ExtractIsoYear.register_lookup(YearLt)\n-ExtractIsoYear.register_lookup(YearLte)\n-\n \n class Now(Func):\n     template = 'CURRENT_TIMESTAMP'\n"}
{"instance_id": "sympy__sympy-13878", "model_name_or_path": "devchain", "model_patch": "diff --git a/sympy/stats/crv_types.py b/sympy/stats/crv_types.py\nindex fff7da80ae..231d3b1068 100644\n--- a/sympy/stats/crv_types.py\n+++ b/sympy/stats/crv_types.py\n@@ -47,7 +47,7 @@\n \n from sympy import (log, sqrt, pi, S, Dummy, Interval, sympify, gamma,\n                    Piecewise, And, Eq, binomial, factorial, Sum, floor, Abs,\n-                   Lambda, Basic, lowergamma, erf, erfc, I)\n+                   Lambda, Basic, lowergamma, uppergamma, erf, erfc, I, asin, hyper)\n from sympy import beta as beta_fn\n from sympy import cos, exp, besseli\n from sympy.stats.crv import (SingleContinuousPSpace, SingleContinuousDistribution,\n@@ -153,6 +153,14 @@ class ArcsinDistribution(SingleContinuousDistribution):\n     def pdf(self, x):\n         return 1/(pi*sqrt((x - self.a)*(self.b - x)))\n \n+    def _cdf(self, x):\n+        a, b = self.a, self.b\n+        return Piecewise(\n+            (S.Zero, x < a),\n+            (S(2)/pi*asin(sqrt((x - a)/(b - a))), x <= b),\n+            (S.One, True)\n+        )\n+\n def Arcsin(name, a=0, b=1):\n     r\"\"\"\n     Create a Continuous Random Variable with an arcsin distribution.\n@@ -670,6 +678,13 @@ def pdf(self, x):\n         p, a, b = self.p, self.a, self.b\n         return a*p/x*((x/b)**(a*p)/(((x/b)**a + 1)**(p + 1)))\n \n+    def _cdf(self, x):\n+        p, a, b = self.p, self.a, self.b\n+        return Piecewise(\n+            ((1 + (x/b)**(-a))**(-p), x > 0),\n+            (S.Zero, True)\n+        )\n+\n \n def Dagum(name, p, a, b):\n     r\"\"\"\n@@ -1042,6 +1057,13 @@ def pdf(self, x):\n         a, s, m = self.a, self.s, self.m\n         return a/s * ((x-m)/s)**(-1-a) * exp(-((x-m)/s)**(-a))\n \n+    def _cdf(self, x):\n+        a, s, m = self.a, self.s, self.m\n+        return Piecewise(\n+            (exp(-((x-m)/s)**(-a)), x >= m),\n+            (S.Zero, True)\n+        )\n+\n def Frechet(name, a, s=1, m=0):\n     r\"\"\"\n     Create a continuous random variable with a Frechet distribution.\n@@ -1111,6 +1133,13 @@ def pdf(self, x):\n     def sample(self):\n         return random.gammavariate(self.k, self.theta)\n \n+    def _cdf(self, x):\n+        k, theta = self.k, self.theta\n+        return Piecewise(\n+            (lowergamma(k, x/theta)/gamma(k), x >= 0),\n+            (S.Zero, True)\n+        )\n+\n \n def Gamma(name, k, theta):\n     r\"\"\"\n@@ -1200,6 +1229,13 @@ def pdf(self, x):\n         a, b = self.a, self.b\n         return b**a/gamma(a) * x**(-a-1) * exp(-b/x)\n \n+    def _cdf(self, x):\n+        a, b = self.a, self.b\n+        return Piecewise(\n+            (uppergamma(a, b/x)/gamma(a), x > 0),\n+            (S.Zero, True)\n+        )\n+\n def GammaInverse(name, a, b):\n     r\"\"\"\n     Create a continuous random variable with an inverse Gamma distribution.\n@@ -1385,6 +1421,14 @@ def pdf(self, x):\n         a, b = self.a, self.b\n         return a * b * x**(a-1) * (1-x**a)**(b-1)\n \n+    def _cdf(self, x):\n+        a, b = self.a, self.b\n+        return Piecewise(\n+            (S.One - (S.One - x**a)**b, And(x >= 0, x <= 1)),\n+            (S.Zero, x < 0),\n+            (S.One, True)\n+        )\n+\n def Kumaraswamy(name, a, b):\n     r\"\"\"\n     Create a Continuous Random Variable with a Kumaraswamy distribution.\n@@ -1445,6 +1489,13 @@ def pdf(self, x):\n         mu, b = self.mu, self.b\n         return 1/(2*b)*exp(-Abs(x - mu)/b)\n \n+    def _cdf(self, x):\n+        mu, b = self.mu, self.b\n+        return Piecewise(\n+            (exp((x - mu)/b)/2, x < mu),\n+            (S.One - exp((mu - x)/b)/2, True)\n+        )\n+\n \n def Laplace(name, mu, b):\n     r\"\"\"\n@@ -1501,6 +1552,10 @@ def pdf(self, x):\n         mu, s = self.mu, self.s\n         return exp(-(x - mu)/s)/(s*(1 + exp(-(x - mu)/s))**2)\n \n+    def _cdf(self, x):\n+        mu, s = self.mu, self.s\n+        return S.One / (S.One + exp(-(x - mu)/s))\n+\n \n def Logistic(name, mu, s):\n     r\"\"\"\n@@ -1711,6 +1766,13 @@ def pdf(self, x):\n         mu, omega = self.mu, self.omega\n         return 2*mu**mu/(gamma(mu)*omega**mu)*x**(2*mu - 1)*exp(-mu/omega*x**2)\n \n+    def _cdf(self, x):\n+        mu, omega = self.mu, self.omega\n+        return Piecewise(\n+            (lowergamma(mu, mu*x**2/omega)/gamma(mu), x >= 0),\n+            (S.Zero, True)\n+        )\n+\n \n def Nakagami(name, mu, omega):\n     r\"\"\"\n@@ -2227,6 +2289,11 @@ def pdf(self, x):\n         nu = self.nu\n         return 1/(sqrt(nu)*beta_fn(S(1)/2, nu/2))*(1 + x**2/nu)**(-(nu + 1)/2)\n \n+    def _cdf(self, x):\n+        nu = self.nu\n+        return S.Half + x*gamma((nu + 1)/2)*hyper([S.Half, (nu + 1)/2],\n+                    [S(3)/2], -x**2/nu)/(sqrt(pi*nu)*gamma(nu/2))\n+\n \n def StudentT(name, nu):\n     r\"\"\"\n@@ -2554,6 +2621,16 @@ def pdf(self, x):\n         return 1/factorial(\n             n - 1)*Sum((-1)**k*binomial(n, k)*(x - k)**(n - 1), (k, 0, floor(x)))\n \n+    def _cdf(self, x):\n+        n = self.n\n+        k = Dummy(\"k\")\n+        return Piecewise(\n+            (1/factorial(n)*Sum((-1)**k*binomial(n, k)*(x - k)**n, (k, 0, floor(x))),\n+             And(x >= 0, x <= n)),\n+            (S.Zero, x < 0),\n+            (S.One, True)\n+        )\n+\n \n \n def UniformSum(name, n):\n"}
{"instance_id": "sphinx-doc__sphinx-10435", "model_name_or_path": "devchain", "model_patch": "diff --git a/sphinx/writers/latex.py b/sphinx/writers/latex.py\nindex e0c7d56f8..f275100e9 100644\n--- a/sphinx/writers/latex.py\n+++ b/sphinx/writers/latex.py\n@@ -1710,11 +1710,11 @@ class LaTeXTranslator(SphinxTranslator):\n         # TODO: Use nowrap option once LaTeX formatter supports it\n         # https://github.com/pygments/pygments/pull/1343\n         hlcode = hlcode.replace(r'\\begin{Verbatim}[commandchars=\\\\\\{\\}]',\n-                                r'\\sphinxcode{\\sphinxupquote{')\n+                                r'\\sphinxcode{\\sphinxupquote{%')\n         # get consistent trailer\n-        hlcode = hlcode.rstrip()[:-14]  # strip \\end{Verbatim}\n+        hlcode = hlcode.rstrip()[:-15]  # strip \\n\\end{Verbatim}\n         self.body.append(hlcode)\n-        self.body.append('}}')\n+        self.body.append('%' + CR + '}}')\n         raise nodes.SkipNode\n \n     def depart_literal(self, node: Element) -> None:\n"}
{"instance_id": "astropy__astropy-13033", "model_name_or_path": "devchain", "model_patch": "diff --git a/astropy/timeseries/core.py b/astropy/timeseries/core.py\nindex c279f35f87..08198fb827 100644\n--- a/astropy/timeseries/core.py\n+++ b/astropy/timeseries/core.py\n@@ -76,9 +76,23 @@ class BaseTimeSeries(QTable):\n \n             elif self.colnames[:len(required_columns)] != required_columns:\n \n-                raise ValueError(\"{} object is invalid - expected '{}' \"\n-                                 \"as the first column{} but found '{}'\"\n-                                 .format(self.__class__.__name__, required_columns[0], plural, self.colnames[0]))\n+                # Check which required columns are missing from the table\n+                missing_columns = [col for col in required_columns if col not in self.colnames]\n+\n+                # If first column is correct but other required columns are missing,\n+                # provide a clear message about missing columns\n+                if missing_columns and self.colnames and self.colnames[0] == required_columns[0]:\n+                    missing_str = ', '.join(repr(col) for col in missing_columns)\n+                    raise ValueError(\"{} object is invalid - time series is missing \"\n+                                     \"required column{}: {}\"\n+                                     .format(self.__class__.__name__, 's' if len(missing_columns) > 1 else '',\n+                                             missing_str))\n+                else:\n+                    # First column is wrong or table is empty\n+                    raise ValueError(\"{} object is invalid - expected '{}' \"\n+                                     \"as the first column{} but found '{}'\"\n+                                     .format(self.__class__.__name__, required_columns[0], plural,\n+                                             self.colnames[0] if len(self.colnames) > 0 else 'no columns'))\n \n             if (self._required_columns_relax\n                     and self._required_columns == self.colnames[:len(self._required_columns)]):\n"}
{"instance_id": "sympy__sympy-19495", "model_name_or_path": "devchain", "model_patch": "diff --git a/sympy/sets/conditionset.py b/sympy/sets/conditionset.py\nindex 118eda6f77..c8c70929d9 100644\n--- a/sympy/sets/conditionset.py\n+++ b/sympy/sets/conditionset.py\n@@ -243,7 +243,7 @@ def _eval_subs(self, old, new):\n         cond = self.condition.subs(old, new)\n         base = self.base_set.subs(old, new)\n         if cond is S.true:\n-            return ConditionSet(new, Contains(new, base), base)\n+            return base\n         return self.func(self.sym, cond, base)\n \n     def dummy_eq(self, other, symbol=None):\n"}
{"instance_id": "django__django-16642", "model_name_or_path": "devchain", "model_patch": "diff --git a/django/http/response.py b/django/http/response.py\nindex f62dec8ce9..17e630be37 100644\n--- a/django/http/response.py\n+++ b/django/http/response.py\n@@ -612,6 +612,8 @@ class FileResponse(StreamingHttpResponse):\n                     \"bzip2\": \"application/x-bzip\",\n                     \"gzip\": \"application/gzip\",\n                     \"xz\": \"application/x-xz\",\n+                    \"compress\": \"application/x-compress\",\n+                    \"br\": \"application/x-brotli\",\n                 }.get(encoding, content_type)\n                 self.headers[\"Content-Type\"] = (\n                     content_type or \"application/octet-stream\"\n"}
